{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#what-is-a-restricted-kirchhoff-machine","title":"What is a Restricted Kirchhoff Machine?","text":"<p>You may be familiar with Restricted Boltzmann Machines (RBMs) [1]-[2], which are a type of generative neural network that can learn a probability distribution over its input data. The Restricted Kirchhoff Machine (RKM) is a realization of a RBM using resistor networks, and Kirchhoff's laws of electrical circuits. In this repository, we provide a Python package to virtually simulate the training and evaluation of RKMs.</p> <p>For more information about the capabilities of the RKM, see the original paper by Link to paper XXXX.</p>"},{"location":"#repository-contents","title":"Repository Contents","text":"<p>In this repository you will find the following:</p> <ul> <li><code>src/pyrkm/</code>: The main package code. You can use this code to train and evaluate RKMs. For more information, see the documentation. For a quick start, see the Usage section below.</li> <li><code>energy_consumption</code>: A series of scripts to evaluate the energy consumption of the RKM and compare it to the estimated cost of a RBM. They are used to generate the results in the paper XXX.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To get started with the project, follow these steps:</p> <ul> <li> <p>Prerequisites:   In order to correctly install <code>pyrkm</code> you need <code>python3.9</code> or higher. If you don't have it installed, you can download it from the official website.</p> </li> <li> <p>Install the package: <pre><code>python -m pip install pyrkm\n</code></pre></p> </li> <li> <p>Or: Clone the repository: <pre><code>git clone https://github.com/Kirchhoff-Machines/pyrkm.git\ncd pyrkm\ngit submodule init\ngit submodule update\npip install .\n</code></pre></p> </li> </ul>"},{"location":"#usage","title":"Usage","text":"<p>To learn how to use the package, follow the official documentation and in particular this tutorial.</p>"},{"location":"#contribution-guidelines","title":"Contribution Guidelines","text":"<p>We welcome contributions to improve and expand the capabilities of this project. If you have ideas, bug fixes, or enhancements, please submit a pull request. Check out our Contributing Guidelines to get started with development.</p>"},{"location":"#generative-ai-disclaimer","title":"Generative-AI Disclaimer","text":"<p>Parts of the code have been generated and/or refined using GitHub Copilot. All AI-output has been verified for correctness, accuracy and completeness, revised where needed, and approved by the author(s).</p>"},{"location":"#how-to-cite","title":"How to cite","text":"<p>Please consider citing this software that is published in Zenodo under the DOI 10.5281/zenodo.14865380.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the Apache 2.0 License - see the LICENSE file for details.</p>"},{"location":"CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at s.ciarella@esciencecenter.nl. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"CODE_OF_CONDUCT/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"CODE_OF_CONDUCT/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"CODE_OF_CONDUCT/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"CODE_OF_CONDUCT/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior,  harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"CONTRIBUTING/","title":"Contributing guidelines","text":"<p>Welcome! pyrkm is an open-source project for the analysis of speckle patterns. If you're trying pyrkm with your data, your experience, questions, bugs you encountered, and suggestions for improvement are important to the success of the project.</p> <p>We have a Code of Conduct, please follow it in all your interactions with the project.</p>"},{"location":"CONTRIBUTING/#questions-feedback-bugs","title":"Questions, feedback, bugs","text":"<p>Use the search function to see if someone else already ran across the same issue. Feel free to open a new issue here to ask a question, suggest improvements/new features, or report any bugs that you ran into.</p>"},{"location":"CONTRIBUTING/#submitting-changes","title":"Submitting changes","text":"<p>Even better than a good bug report is a fix for the bug or the implementation of a new feature. We welcome any contributions that help improve the code.</p> <p>When contributing to this repository, please first discuss the change you wish to make via an issue with the owners of this repository before making a change.</p> <p>Contributions can come in the form of:</p> <ul> <li>Bug fixes</li> <li>New features</li> <li>Improvement of existing code</li> <li>Updates to the documentation</li> <li>... ?</li> </ul> <p>We use the usual GitHub pull request flow. For more info see GitHub's own documentation.</p> <p>Typically this means:</p> <ol> <li>Forking the repository and/or make a new branch</li> <li>Making your changes</li> <li>Make sure that the tests pass and add your own</li> <li>Update the documentation is updated for new features</li> <li>Pushing the code back to Github</li> <li>Create a new Pull Request</li> </ol> <p>One of the code owners will review your code and request changes if needed. Once your changes have been approved, your contributions will become part of pyrkm. \ud83c\udf89</p>"},{"location":"CONTRIBUTING/#getting-started-with-development","title":"Getting started with development","text":""},{"location":"CONTRIBUTING/#setup","title":"Setup","text":"<p>pyrkm targets Python 3.9 or newer.</p> <p>Clone the repository into the <code>pyrkm</code> directory:</p> <pre><code>git clone https://github.com/Kirchhoff-Machines/pyrkm.git\ncd pyrkm\n</code></pre> <p>Initialize all submodules: <pre><code>git submodule update --recursive --init\n</code></pre></p> <p>Install using <code>virtualenv</code>:</p> <pre><code>python3 -m venv env\nsource env/bin/activate\npython3 -m pip install -e .[develop]\n</code></pre> <p>Alternatively, install using Conda:</p> <pre><code>conda create -n pyrkm python=3.10\nconda activate pyrkm\npip install -e .[develop]\n</code></pre>"},{"location":"CONTRIBUTING/#running-tests","title":"Running tests","text":"<p>pyrkm uses pytest to run the tests. You can run the tests for yourself using:</p> <pre><code>pytest\n</code></pre> <p>Notice that some of the tests will fail the first time that you run them locally. After you get this failure message you can run</p> <pre><code>python ./scripts/setup_test.py\n</code></pre> <p>to stash the test data in the correct location. After that, you can run the tests again and they should pass.</p> <p>To check coverage:</p> <pre><code>coverage run -m pytest\ncoverage report  # to output to terminal\ncoverage html    # to generate html report\n</code></pre>"},{"location":"CONTRIBUTING/#building-the-documentation","title":"Building the documentation","text":"<p>The documentation is written in markdown, and uses mkdocs to generate the pages.</p> <p>To build the documentation for yourself:</p> <pre><code>pip install -e .[docs]\nmkdocs serve\n</code></pre> <p>You can find the documentation source in the docs directory. If you are adding new pages, make sure to update the listing in the <code>mkdocs.yml</code> under the <code>nav</code> entry.</p>"},{"location":"CONTRIBUTING/#making-a-release","title":"Making a release","text":"<ol> <li> <p>Make a new release.</p> </li> <li> <p>Under 'Choose a tag', set the tag to the new version. The versioning scheme we use is SemVer, so bump the version (major/minor/patch) as needed. Bumping the version is handled transparently by <code>bumpversion</code> in this workflow.</p> </li> <li> <p>The upload to pypi is triggered when a release is published and handled by this workflow.</p> </li> <li> <p>The upload to zenodo is triggered when a release is published.</p> </li> </ol>"},{"location":"energy_consumption/","title":"Energy Consumption","text":""},{"location":"energy_consumption/#overview","title":"Overview","text":"<p>This documentation provides an overview of the <code>energy_consumption</code> submodule and its components. The submodule is designed to handle various measurements related to energy consumption and to produce the data reported in the paper XXX.</p> <p>For the purpose of the study XXX, we consider only the energy consumed by: (i) the circuit structure of the RKM, which can be measured analytically using Kirchhoff's laws, and (ii) the matrix multiplication on a computer, the simplest operation behind the RKM, for which we introduce specific tools in this submodule.</p>"},{"location":"energy_consumption/#tools-and-limitations","title":"Tools and Limitations","text":"<p>To capture the energy consumed by the computer, we use the following tools:</p> <ul> <li> <p><code>pyRAPL</code>: a Python package that uses Intel's \u201cRunning Average Power Limit\u201d (RAPL) technology to estimate CPU power consumption. This technology is available on Intel CPUs since the Sandy Bridge generation.</p> </li> <li> <p><code>nvidia-smi</code>: the standard NVIDIA System Management Interface to measure GPU energy consumption.</p> </li> </ul> <p>This submodule can measure the energy consumption of both the CPU and the GPU, but it is limited to Intel CPUs and NVIDIA GPUs.</p>"},{"location":"energy_consumption/#content-of-the-submodule","title":"Content of the Submodule","text":"<p>In the <code>src/</code> directory, you will find several files to analyze the energy consumption of matrix multiplication as a function of the number of hidden nodes <code>Nh</code> of the machine. Running these codes will produce images in <code>out_png</code> and store data in <code>out_csv</code>. We include the images and data used to produce the results in XXX.</p> <p>The specific files for the analysis are:</p>"},{"location":"energy_consumption/#measure_consumptionpy","title":"<code>measure_consumption.py</code>","text":"<p>Measures the energy consumption and power usage of matrix-vector multiplications using PyTorch on a CPU, varying the size of the vectors. It plots and saves the results as a CSV file and a PNG image.</p>"},{"location":"energy_consumption/#nvidiasmi_gpupy","title":"<code>nvidiasmi_gpu.py</code>","text":"<p>Measures the energy consumption and power usage of matrix-vector multiplications using PyTorch on a GPU, varying the size of the vectors. It also measures CPU energy if enabled, and plots and saves the results as a CSV file and a PNG image.</p>"},{"location":"energy_consumption/#estimate_cpu_scalingpy","title":"<code>estimate_cpu_scaling.py</code>","text":"<p>Reads energy and time data from a CSV file, fits polynomial models to the data, and extrapolates to predict values for larger sizes (<code>Nh</code>). It plots the original and extrapolated data, saves the plot as a PNG file, and exports the estimated values to a new CSV file.</p>"},{"location":"energy_consumption/#estimate_gpu_scalingpy","title":"<code>estimate_gpu_scaling.py</code>","text":"<p>Reads GPU and CPU energy data from a CSV file, fits polynomial models to the time and total energy data, and extrapolates to predict values for larger sizes (<code>Nh</code>). It plots the original and extrapolated data, saves the plot as a PNG file, and exports the estimated values to a new CSV file.</p>"},{"location":"energy_consumption/#conclusion","title":"Conclusion","text":"<p>The <code>energy_consumption</code> submodule provides tools to measure and analyze the energy consumption of matrix multiplications on both CPUs and GPUs. By using <code>pyRAPL</code> and <code>nvidia-smi</code>, we can capture detailed energy usage data, which is essential for understanding the efficiency of the RKM's operations. The provided scripts facilitate the analysis and visualization of this data, enabling further research and optimization.</p>"},{"location":"installation/","title":"Installation","text":"<p>To get started with the project, follow these steps:</p> <ul> <li> <p>Prerequisites:   In order to correctly install <code>pyrkm</code> you need <code>python3.9</code> or higher. If you don't have it installed, you can download it from the official website.</p> </li> <li> <p>Install the package: <pre><code>python -m pip install pyrkm\n</code></pre></p> </li> <li> <p>Or: Clone the repository: <pre><code>git clone https://github.com/Kirchhoff-Machines/pyrkm.git\ncd pyrkm\ngit submodule init\ngit submodule update\npip install .\n</code></pre></p> </li> </ul>"},{"location":"api/api/","title":"pyrkm","text":"<p>The main API of the <code>pyrkm</code> package is composed of the following modules:</p> <ul> <li> <p>pyrkm.rbm: contains the class corresponding to standard RBMs with corresponding methods for training and analysis.</p> </li> <li> <p>pyrkm.rkm: contains the class corresponding to the Restricted Kirchhoff Machine (RKM) with corresponding methods for training and analysis.</p> </li> <li> <p>pyrkm.utils: contains utility functions for data processing and visualization.</p> </li> <li> <p>pyrkm.circuit_utils: contains utility functions for computations related to electrical circuits.</p> </li> <li> <p>pyrkm.classifier: contains the class corresponding to a simple CNN classifier, used in some of the utils.</p> </li> </ul>"},{"location":"api/circuit_utils/","title":"circuit_utils","text":""},{"location":"api/circuit_utils/#pyrkm.circuit_utils.Circuit","title":"<code>Circuit(graph)</code>","text":"<p>               Bases: <code>object</code></p> <p>Class to simulate a circuit with trainable conductances</p> <p>Parameters:</p> <ul> <li> <code>graph</code>               (<code>str or Graph</code>)           \u2013            <p>If str, it is the path to the file containing the graph. If networkx.Graph, it is the graph itself.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>graph</code>               (<code>Graph</code>)           \u2013            <p>Graph specifying the nodes and edges in the network. A conductance parameter is associated with each edge. A trainable edge will be updated during training.</p> </li> <li> <code>n</code>               (<code>int</code>)           \u2013            <p>Number of nodes in the graph.</p> </li> <li> <code>ne</code>               (<code>int</code>)           \u2013            <p>Number of edges in the graph.</p> </li> <li> <code>pts</code>               (<code>ndarray</code>)           \u2013            <p>Positions of the nodes in the graph.</p> </li> </ul> Source code in <code>src/pyrkm/circuit_utils.py</code> <pre><code>def __init__(self, graph):\n    if isinstance(graph, str):\n        self.graph = nx.read_gpickle(graph)\n    else:\n        self.graph = graph\n\n    self.n = len(self.graph.nodes)\n    self.ne = len(self.graph.edges)\n    self.pts = np.array(\n        [self.graph.nodes[node]['pos'] for node in graph.nodes])\n    self.incidence_matrix = nx.incidence_matrix(self.graph, oriented=True)\n</code></pre>"},{"location":"api/circuit_utils/#pyrkm.circuit_utils.Circuit.constraint_matrix","title":"<code>constraint_matrix(indices_nodes)</code>","text":"<p>Compute the constraint matrix Q for the circuit and the nodes represented by indices_nodes.</p> <p>Parameters:</p> <ul> <li> <code>indices_nodes</code>               (<code>ndarray</code>)           \u2013            <p>Array with the indices of the nodes to be constrained. The nodes themselves are given by np.array(self.graph.nodes)[indices_nodes].</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>csr_matrix</code>           \u2013            <p>Constraint matrix Q: a sparse constraint rectangular matrix of size n x len(indices_nodes). Its entries are only 1 or 0. Q.Q^T is a projector onto to the space of the nodes.</p> </li> </ul> Source code in <code>src/pyrkm/circuit_utils.py</code> <pre><code>def constraint_matrix(self, indices_nodes):\n    ''' Compute the constraint matrix Q for the circuit and the nodes represented by indices_nodes.\n\n    Parameters\n    ----------\n    indices_nodes : numpy.ndarray\n        Array with the indices of the nodes to be constrained.\n        The nodes themselves are given by\n        np.array(self.graph.nodes)[indices_nodes].\n\n    Returns\n    -------\n    scipy.sparse.csr_matrix\n        Constraint matrix Q: a sparse constraint rectangular\n        matrix of size n x len(indices_nodes).\n        Its entries are only 1 or 0.\n        Q.Q^T is a projector onto to the space of the nodes.\n    '''\n    if len(indices_nodes) == 0:\n        raise ValueError('indicesNodes must be a non-empty array.')\n    Q = csr_matrix((np.ones(len(indices_nodes)),\n                    (indices_nodes, np.arange(len(indices_nodes)))),\n                   shape=(self.n, len(indices_nodes)))\n    return Q\n</code></pre>"},{"location":"api/circuit_utils/#pyrkm.circuit_utils.Circuit.plot_edge_state","title":"<code>plot_edge_state(edge_state, title=None, lw=0.5, cmap='RdYlBu_r')</code>","text":"<p>Plot the state of the edges in the graph.</p> <p>Parameters:</p> <ul> <li> <code>edge_state</code>               (<code>ndarray</code>)           \u2013            <p>State of the edges in the graph. edge_state has size ne.</p> </li> <li> <code>title</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Title of the plot.</p> </li> <li> <code>lw</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Line width of the edges.</p> </li> <li> <code>cmap</code>               (<code>str</code>, default:                   <code>'RdYlBu_r'</code> )           \u2013            <p>Colormap to use for the plot.</p> </li> </ul> Source code in <code>src/pyrkm/circuit_utils.py</code> <pre><code>def plot_edge_state(self, edge_state, title=None, lw=0.5, cmap='RdYlBu_r'):\n    ''' Plot the state of the edges in the graph.\n\n    Parameters\n    ----------\n    edge_state : numpy.ndarray\n        State of the edges in the graph. edge_state has size ne.\n    title : str, optional\n        Title of the plot.\n    lw : float, optional\n        Line width of the edges.\n    cmap : str, optional\n        Colormap to use for the plot.\n    '''\n    _cmap = plt.cm.get_cmap(cmap)\n    pos_edges = np.array([\n        np.array([\n            self.graph.nodes[edge[0]]['pos'],\n            self.graph.nodes[edge[1]]['pos']\n        ]).T for edge in self.graph.edges()\n    ])\n    norm = plt.Normalize(vmin=np.min(edge_state), vmax=np.max(edge_state))\n    fig, axs = plt.subplots(1, 1, figsize=(4, 4))\n    for i in range(len(pos_edges)):\n        axs.plot(pos_edges[i, 0],\n                 pos_edges[i, 1],\n                 color=_cmap(norm(edge_state[i])))\n    axs.set_xticks([])\n    axs.set_yticks([])\n    fig.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=cmap),\n                 ax=axs,\n                 shrink=0.5)\n    axs.set_title(title)\n</code></pre>"},{"location":"api/circuit_utils/#pyrkm.circuit_utils.Circuit.plot_node_state","title":"<code>plot_node_state(node_state, title=None, lw=0.5, cmap='RdYlBu_r', size_factor=100)</code>","text":"<p>Plot the state of the nodes in the graph.</p> <p>Parameters:</p> <ul> <li> <code>node_state</code>               (<code>ndarray</code>)           \u2013            <p>State of the nodes in the graph. node_state has size n.</p> </li> <li> <code>title</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Title of the plot.</p> </li> <li> <code>lw</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Line width of the edges.</p> </li> <li> <code>cmap</code>               (<code>str</code>, default:                   <code>'RdYlBu_r'</code> )           \u2013            <p>Colormap to use for the plot.</p> </li> <li> <code>size_factor</code>               (<code>float</code>, default:                   <code>100</code> )           \u2013            <p>Factor to scale the size of the nodes.</p> </li> </ul> Source code in <code>src/pyrkm/circuit_utils.py</code> <pre><code>def plot_node_state(self,\n                    node_state,\n                    title=None,\n                    lw=0.5,\n                    cmap='RdYlBu_r',\n                    size_factor=100):\n    ''' Plot the state of the nodes in the graph.\n\n    Parameters\n    ----------\n    node_state : numpy.ndarray\n        State of the nodes in the graph. node_state has size n.\n    title : str, optional\n        Title of the plot.\n    lw : float, optional\n        Line width of the edges.\n    cmap : str, optional\n        Colormap to use for the plot.\n    size_factor : float, optional\n        Factor to scale the size of the nodes.\n    '''\n    posX = self.pts[:, 0]\n    posY = self.pts[:, 1]\n    norm = plt.Normalize(vmin=np.min(node_state), vmax=np.max(node_state))\n    fig, axs = plt.subplots(1,\n                            1,\n                            figsize=(4, 4),\n                            constrained_layout=True,\n                            sharey=True)\n    axs.scatter(posX,\n                posY,\n                s=size_factor * np.abs(node_state[:]),\n                c=node_state[:],\n                edgecolors='black',\n                linewidth=lw,\n                cmap=cmap,\n                norm=norm)\n    axs.set(aspect='equal')\n    axs.set_xticks([])\n    axs.set_yticks([])\n    fig.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=cmap),\n                 ax=axs,\n                 shrink=0.5)\n    axs.set_title(title)\n</code></pre>"},{"location":"api/circuit_utils/#pyrkm.circuit_utils.Circuit.setConductances","title":"<code>setConductances(conductances)</code>","text":"<p>Set the conductances of the edges in the graph.</p> <p>Parameters:</p> <ul> <li> <code>conductances</code>               (<code>list of float or numpy.ndarray</code>)           \u2013            <p>Conductances of the edges. Must have the same length as the number of edges.</p> </li> </ul> Source code in <code>src/pyrkm/circuit_utils.py</code> <pre><code>def setConductances(self, conductances):\n    ''' Set the conductances of the edges in the graph.\n\n    Parameters\n    ----------\n    conductances : list of float or numpy.ndarray\n        Conductances of the edges. Must have the same length as the number of edges.\n    '''\n    assert len(\n        conductances\n    ) == self.ne, 'conductances must have the same length as the number of edges'\n    if isinstance(conductances, list):\n        conductances = np.array(conductances)\n    self.conductances = conductances\n</code></pre>"},{"location":"api/circuit_utils/#pyrkm.circuit_utils.Circuit.solve","title":"<code>solve(Q, f)</code>","text":"<p>Solve the circuit with the constraint matrix Q and the source vector f.</p> <p>Parameters:</p> <ul> <li> <code>Q</code>               (<code>csr_matrix</code>)           \u2013            <p>Constraint matrix Q</p> </li> <li> <code>f</code>               (<code>ndarray</code>)           \u2013            <p>Source vector f. f has size len(indices_nodes).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>Solution vector V. V has size n.</p> </li> </ul> Source code in <code>src/pyrkm/circuit_utils.py</code> <pre><code>def solve(self, Q, f):\n    ''' Solve the circuit with the constraint matrix Q and the source vector f.\n\n    Parameters\n    ----------\n    Q : scipy.sparse.csr_matrix\n        Constraint matrix Q\n    f : numpy.ndarray\n        Source vector f. f has size len(indices_nodes).\n\n    Returns\n    -------\n    numpy.ndarray\n        Solution vector V. V has size n.\n    '''\n    try:\n        self.conductances\n    except AttributeError:\n        raise AttributeError('Conductances have not been set yet.')\n    if len(f) != Q.shape[1]:\n        raise ValueError('Source vector f has the wrong size.')\n    H = self._extended_hessian(Q)\n    f_extended = np.hstack([np.zeros(self.n), f])\n    V = spsolve(H, f_extended)[:self.n]\n    return V\n</code></pre>"},{"location":"api/classifier/","title":"classifier","text":""},{"location":"api/classifier/#pyrkm.classifier.CustomDataset","title":"<code>CustomDataset(data, targets, transform=None)</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Custom Dataset for loading data and targets.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>array - like</code>)           \u2013            <p>The data samples.</p> </li> <li> <code>targets</code>               (<code>array - like</code>)           \u2013            <p>The target labels.</p> </li> <li> <code>transform</code>               (<code>callable</code>, default:                   <code>None</code> )           \u2013            <p>A function/transform to apply to the data.</p> </li> </ul> Source code in <code>src/pyrkm/classifier.py</code> <pre><code>def __init__(self, data, targets, transform=None):\n    self.data = data\n    self.targets = targets\n    self.transform = transform\n</code></pre>"},{"location":"api/classifier/#pyrkm.classifier.SimpleClassifier","title":"<code>SimpleClassifier()</code>","text":"<p>               Bases: <code>Module</code></p> <p>A simple Convolutional Neural Network (CNN) for classification.</p> <p>Methods:</p> <ul> <li> <code>forward</code>             \u2013              <p>Defines the forward pass of the model.</p> </li> </ul> Source code in <code>src/pyrkm/classifier.py</code> <pre><code>def __init__(self):\n    super(SimpleClassifier, self).__init__()\n    self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n    self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n    self.fc1 = nn.Linear(64 * 7 * 7, 128)\n    self.fc2 = nn.Linear(128, 10)\n</code></pre>"},{"location":"api/classifier/#pyrkm.classifier.show_classification","title":"<code>show_classification(file_name, img, cmap='gray', vmin=None, vmax=None, save=False, savename='', labels=None)</code>","text":"<p>Display an image or a grid of images with optional labels and save if specified.</p> <p>Parameters:</p> <ul> <li> <code>file_name</code>               (<code>str</code>)           \u2013            <p>Title for the plot.</p> </li> <li> <code>img</code>               (<code>numpy array</code>)           \u2013            <p>Image or grid of images to display.</p> </li> <li> <code>cmap</code>               (<code>str</code>, default:                   <code>'gray'</code> )           \u2013            <p>Colormap to use for <code>plt.imshow</code> (default is 'gray').</p> </li> <li> <code>vmin</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Minimum data value that corresponds to colormap 'under' value.</p> </li> <li> <code>vmax</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Maximum data value that corresponds to colormap 'over' value.</p> </li> <li> <code>save</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, saves the plot to <code>savename</code> (default is False).</p> </li> <li> <code>savename</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>Filename to save the plot (default is '').</p> </li> <li> <code>labels</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>Optional 1D list of labels for a grid of images.</p> </li> </ul> Source code in <code>src/pyrkm/classifier.py</code> <pre><code>def show_classification(file_name,\n                        img,\n                        cmap='gray',\n                        vmin=None,\n                        vmax=None,\n                        save=False,\n                        savename='',\n                        labels=None):\n    \"\"\"Display an image or a grid of images with optional labels and save if specified.\n\n    Parameters\n    ----------\n    file_name : str\n        Title for the plot.\n    img : numpy array\n        Image or grid of images to display.\n    cmap : str, optional\n        Colormap to use for `plt.imshow` (default is 'gray').\n    vmin : float, optional\n        Minimum data value that corresponds to colormap 'under' value.\n    vmax : float, optional\n        Maximum data value that corresponds to colormap 'over' value.\n    save : bool, optional\n        If True, saves the plot to `savename` (default is False).\n    savename : str, optional\n        Filename to save the plot (default is '').\n    labels : list, optional\n        Optional 1D list of labels for a grid of images.\n    \"\"\"\n    plt.figure(figsize=(8, 8))\n    plt.title(file_name)\n    plt.imshow(img, cmap=cmap, vmin=vmin, vmax=vmax)\n\n    # Overlay labels if provided\n    if labels is not None:\n        num_images = len(labels)\n        grid_size = int(num_images**0.5)\n\n        # Ensure labels length matches the grid size\n        if grid_size * grid_size != num_images:\n            raise ValueError('The length of labels must be a perfect square.')\n\n        # Calculate sub-image dimensions\n        img_height, img_width = img.shape[0] // grid_size, img.shape[\n            1] // grid_size\n\n        # Place labels\n        for idx, label in enumerate(labels):\n            row, col = divmod(idx, grid_size)\n            plt.text(\n                col * img_width + img_width,  # x-coordinate\n                row * img_height,  # y-coordinate\n                label,  # Label text\n                color='red',\n                fontsize=10,\n                ha='center',\n                va='center',\n                bbox=dict(facecolor='white', alpha=0.6, edgecolor='none'))\n\n    if save:\n        plt.savefig(savename)\n        plt.close()\n    else:\n        plt.show()\n</code></pre>"},{"location":"api/classifier/#pyrkm.classifier.train_classifier","title":"<code>train_classifier(test_set, train_set, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'), batch_size=64, learning_rate=0.001, num_epochs=5)</code>","text":"<p>Train a simple classifier on the provided dataset.</p> <p>Parameters:</p> <ul> <li> <code>test_set</code>               (<code>Tuple</code>)           \u2013            <p>The test dataset as a tuple (data, targets).</p> </li> <li> <code>train_set</code>               (<code>Tuple</code>)           \u2013            <p>The training dataset as a tuple (data, targets).</p> </li> <li> <code>device</code>               (<code>device</code>, default:                   <code>device('cuda' if is_available() else 'cpu')</code> )           \u2013            <p>The device to run the training on (default is CUDA if available).</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>64</code> )           \u2013            <p>The batch size for training (default is 64).</p> </li> <li> <code>learning_rate</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>The learning rate for the optimizer (default is 0.001).</p> </li> <li> <code>num_epochs</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>The number of epochs to train for (default is 5).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>model</code> (              <code>Module</code> )          \u2013            <p>The trained model.</p> </li> <li> <code>accuracy</code> (              <code>float</code> )          \u2013            <p>The accuracy of the model on the test set.</p> </li> </ul> Source code in <code>src/pyrkm/classifier.py</code> <pre><code>def train_classifier(test_set: Tuple,\n                     train_set: Tuple,\n                     device: torch.device = torch.device(\n                         'cuda' if torch.cuda.is_available() else 'cpu'),\n                     batch_size: int = 64,\n                     learning_rate: float = 0.001,\n                     num_epochs: int = 5) -&gt; Tuple[nn.Module, float]:\n    \"\"\"Train a simple classifier on the provided dataset.\n\n    Parameters\n    ----------\n    test_set : Tuple\n        The test dataset as a tuple (data, targets).\n    train_set : Tuple\n        The training dataset as a tuple (data, targets).\n    device : torch.device, optional\n        The device to run the training on (default is CUDA if available).\n    batch_size : int, optional\n        The batch size for training (default is 64).\n    learning_rate : float, optional\n        The learning rate for the optimizer (default is 0.001).\n    num_epochs : int, optional\n        The number of epochs to train for (default is 5).\n\n    Returns\n    -------\n    model : nn.Module\n        The trained model.\n    accuracy : float\n        The accuracy of the model on the test set.\n    \"\"\"\n    train_dataset = CustomDataset(train_set[0], train_set[1])\n    test_dataset = CustomDataset(test_set[0], test_set[1])\n\n    # Create data loaders\n    train_loader = DataLoader(dataset=train_dataset,\n                              batch_size=batch_size,\n                              shuffle=True)\n    test_loader = DataLoader(dataset=test_dataset,\n                             batch_size=batch_size,\n                             shuffle=False)\n\n    # Initialize the model, loss function, and optimizer\n    model = SimpleClassifier().to(device).to(train_set[0][0].dtype)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    # Create directory for saving model states\n    os.makedirs('classifier_states', exist_ok=True)\n\n    # Load the latest model state if it exists\n    start_epoch = 0\n    for epoch in range(num_epochs, 0, -1):\n        model_path = f'classifier_states/model_epoch_{epoch}.pth'\n        if os.path.exists(model_path):\n            checkpoint = torch.load(model_path)\n            model.load_state_dict(checkpoint['model_state_dict'])\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            start_epoch = epoch\n            print(f'Resuming training from epoch {start_epoch}')\n            break\n\n    # Training loop\n    for epoch in range(start_epoch, num_epochs):\n        model.train()\n        running_loss = 0.0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            images = images.reshape(-1, 1, 28, 28)\n\n            # one-hot encode the labels\n            labels = nn.functional.one_hot(labels, num_classes=10).to(\n                torch.float32).squeeze()\n\n            # Forward pass\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            # Backward pass and optimization\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n        print(\n            f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}'\n        )\n\n        # Save the model state\n        torch.save(\n            {\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            }, f'classifier_states/model_epoch_{epoch+1}.pth')\n\n    # Evaluation\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            images = images.reshape(-1, 1, 28, 28)\n            labels = labels.reshape(-1)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n\n    return model, 100 * correct / total\n</code></pre>"},{"location":"api/rbm/","title":"RBM class","text":""},{"location":"api/rbm/#pyrkm.rbm.RBM","title":"<code>RBM(model_name, n_visible, n_hidden, k=1, lr=0.001, max_epochs=200000, energy_type='hopfield', optimizer='SGD', regularization=False, l1_factor=0, l2_factor=0.001, g_v=0.5, g_h=0.5, batch_size=1, train_algo='vRDM', centering=False, average_data=None, model_beta=1, mytype=torch.float32, min_W=-10, max_W=10)</code>  <code>dataclass</code>","text":"<p>A class to represent a Restricted Boltzmann Machine (RBM).</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>The name of the model.</p> </li> <li> <code>n_visible</code>               (<code>int</code>)           \u2013            <p>The number of visible units.</p> </li> <li> <code>n_hidden</code>               (<code>int</code>)           \u2013            <p>The number of hidden units.</p> </li> <li> <code>k</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of Gibbs sampling steps (default is 1).</p> </li> <li> <code>lr</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>The learning rate.</p> </li> <li> <code>max_epochs</code>               (<code>int</code>, default:                   <code>200000</code> )           \u2013            <p>The maximum number of training epochs.</p> </li> <li> <code>energy_type</code>               (<code>str</code>, default:                   <code>'hopfield'</code> )           \u2013            <p>The type of energy function to use (default is 'hopfield').</p> </li> <li> <code>optimizer</code>               (<code>str</code>, default:                   <code>'SGD'</code> )           \u2013            <p>The optimizer to use (default is 'SGD', but also Adam is available).</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The batch size for training (default is 1).</p> </li> <li> <code>train_algo</code>               (<code>str</code>, default:                   <code>'vRDM'</code> )           \u2013            <p>The training algorithm to use between Contrastive Divergence (CD), Persistent Contrastive Divergence (PCD), visible-random (default, vRDM), hidden-random (hRDM).</p> </li> <li> <code>average_data</code>               (<code>tensor</code>, default:                   <code>None</code> )           \u2013            <p>The average data tensor for centering and initialization (default is None).</p> </li> <li> <code>model_beta</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The inverse temperature parameter (default is 1).</p> </li> <li> <code>mytype</code>               (<code>type</code>, default:                   <code>float32</code> )           \u2013            <p>The data type for tensors (default is torch.float32).</p> </li> <li> <code>min_W</code>               (<code>float</code>, default:                   <code>-10</code> )           \u2013            <p>The minimum weight value used for clipping (default is -10).</p> </li> <li> <code>max_W</code>               (<code>float</code>, default:                   <code>10</code> )           \u2013            <p>The maximum weight value used for clipping (default is 10).</p> </li> <li> <code>regularization</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use L1+L2 regularization (default is False).</p> </li> <li> <code>l1_factor</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>The L1 regularization factor.</p> </li> <li> <code>l2_factor</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>The L2 regularization factor.</p> </li> <li> <code>centering</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use centering (default is False).</p> </li> <li> <code>g_v</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The visible unit gain, required for gradient centering (default is 0.5).</p> </li> <li> <code>g_h</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The hidden unit gain, required for gradient centering (default is 0.5).</p> </li> </ul>"},{"location":"api/rbm/#pyrkm.rbm.RBM.Adam_update","title":"<code>Adam_update(t, dEdW_data, dEdW_model, dEdv_bias_data, dEdv_bias_model, dEdh_bias_data, dEdh_bias_model)</code>","text":"<p>Updates the model parameters using the Adam optimizer.</p> <p>Parameters:</p> <ul> <li> <code>t</code>               (<code>int</code>)           \u2013            <p>The current epoch.</p> </li> <li> <code>dEdW_data</code>               (<code>Tensor</code>)           \u2013            <p>The gradient of the energy with respect to the weights from the data.</p> </li> <li> <code>dEdW_model</code>               (<code>Tensor</code>)           \u2013            <p>The gradient of the energy with respect to the weights from the model.</p> </li> <li> <code>dEdv_bias_data</code>               (<code>Tensor</code>)           \u2013            <p>The gradient of the energy with respect to the visible biases from the data.</p> </li> <li> <code>dEdv_bias_model</code>               (<code>Tensor</code>)           \u2013            <p>The gradient of the energy with respect to the visible biases from the model.</p> </li> <li> <code>dEdh_bias_data</code>               (<code>Tensor</code>)           \u2013            <p>The gradient of the energy with respect to the hidden biases from the data.</p> </li> <li> <code>dEdh_bias_model</code>               (<code>Tensor</code>)           \u2013            <p>The gradient of the energy with respect to the hidden biases from the model.</p> </li> </ul> Source code in <code>src/pyrkm/rbm.py</code> <pre><code>def Adam_update(self, t, dEdW_data, dEdW_model, dEdv_bias_data,\n                dEdv_bias_model, dEdh_bias_data, dEdh_bias_model):\n    \"\"\"Updates the model parameters using the Adam optimizer.\n\n    Parameters\n    ----------\n    t : int\n        The current epoch.\n    dEdW_data : torch.Tensor\n        The gradient of the energy with respect to the weights from the data.\n    dEdW_model : torch.Tensor\n        The gradient of the energy with respect to the weights from the model.\n    dEdv_bias_data : torch.Tensor\n        The gradient of the energy with respect to the visible biases from the data.\n    dEdv_bias_model : torch.Tensor\n        The gradient of the energy with respect to the visible biases from the model.\n    dEdh_bias_data : torch.Tensor\n        The gradient of the energy with respect to the hidden biases from the data.\n    dEdh_bias_model : torch.Tensor\n        The gradient of the energy with respect to the hidden biases from the model.\n    \"\"\"\n    dW = -dEdW_data + dEdW_model\n    dv = -dEdv_bias_data + dEdv_bias_model\n    dh = -dEdh_bias_data + dEdh_bias_model\n    if self.centering:\n        dv = dv - torch.matmul(self.oh, dW)\n        dh = dh - torch.matmul(self.ov, dW.t())\n    if self.regularization == 'l2':\n        dW += self.l2 * 2 * self.W\n        dv += self.l2 * 2 * self.v_bias\n        dh += self.l2 * 2 * self.h_bias\n    elif self.regularization == 'l1':\n        dW += self.l1 * torch.sign(self.W)\n        dv += self.l1 * torch.sign(self.v_bias)\n        dh += self.l1 * torch.sign(self.h_bias)\n    self.m_dW = self.beta1 * self.m_dW + (1 - self.beta1) * dW\n    self.m_dv = self.beta1 * self.m_dv + (1 - self.beta1) * dv\n    self.m_dh = self.beta1 * self.m_dh + (1 - self.beta1) * dh\n    self.v_dW = self.beta2 * self.v_dW + (1 - self.beta2) * (dW**2)\n    self.v_dv = self.beta2 * self.v_dv + (1 - self.beta2) * (dv**2)\n    self.v_dh = self.beta2 * self.v_dh + (1 - self.beta2) * (dh**2)\n    m_dW_corr = self.m_dW / (1 - self.beta1**t)\n    m_dv_corr = self.m_dv / (1 - self.beta1**t)\n    m_dh_corr = self.m_dh / (1 - self.beta1**t)\n    v_dW_corr = self.v_dW / (1 - self.beta2**t)\n    v_dv_corr = self.v_dv / (1 - self.beta2**t)\n    v_dh_corr = self.v_dh / (1 - self.beta2**t)\n    self.W = self.W + self.lr * (m_dW_corr /\n                                 (torch.sqrt(v_dW_corr) + self.epsilon))\n    self.v_bias = self.v_bias + self.lr * (\n        m_dv_corr / (torch.sqrt(v_dv_corr) + self.epsilon))\n    self.h_bias = self.h_bias + self.lr * (\n        m_dh_corr / (torch.sqrt(v_dh_corr) + self.epsilon))\n</code></pre>"},{"location":"api/rbm/#pyrkm.rbm.RBM.Bernoulli_h_to_v","title":"<code>Bernoulli_h_to_v(h, beta)</code>","text":"<p>Converts hidden units to visible units using Bernoulli sampling.</p> <p>Parameters:</p> <ul> <li> <code>h</code>               (<code>Tensor</code>)           \u2013            <p>The hidden units.</p> </li> <li> <code>beta</code>               (<code>float</code>)           \u2013            <p>The inverse temperature parameter.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple</code>           \u2013            <p>The probabilities and samples of the visible units.</p> </li> </ul> Source code in <code>src/pyrkm/rbm.py</code> <pre><code>def Bernoulli_h_to_v(self, h, beta):\n    \"\"\"Converts hidden units to visible units using Bernoulli sampling.\n\n    Parameters\n    ----------\n    h : torch.Tensor\n        The hidden units.\n    beta : float\n        The inverse temperature parameter.\n\n    Returns\n    -------\n    tuple\n        The probabilities and samples of the visible units.\n    \"\"\"\n    p_v = self._prob_v_given_h(h, beta)\n    sample_v = torch.bernoulli(p_v)\n    return p_v, sample_v\n</code></pre>"},{"location":"api/rbm/#pyrkm.rbm.RBM.Bernoulli_v_to_h","title":"<code>Bernoulli_v_to_h(v, beta)</code>","text":"<p>Converts visible units to hidden units using Bernoulli sampling.</p> <p>Parameters:</p> <ul> <li> <code>v</code>               (<code>Tensor</code>)           \u2013            <p>The visible units.</p> </li> <li> <code>beta</code>               (<code>float</code>)           \u2013            <p>The inverse temperature parameter.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple</code>           \u2013            <p>The probabilities and samples of the hidden units.</p> </li> </ul> Source code in <code>src/pyrkm/rbm.py</code> <pre><code>def Bernoulli_v_to_h(self, v, beta):\n    \"\"\"Converts visible units to hidden units using Bernoulli sampling.\n\n    Parameters\n    ----------\n    v : torch.Tensor\n        The visible units.\n    beta : float\n        The inverse temperature parameter.\n\n    Returns\n    -------\n    tuple\n        The probabilities and samples of the hidden units.\n    \"\"\"\n    p_h = self._prob_h_given_v(v, beta)\n    sample_h = torch.bernoulli(p_h)\n    return p_h, sample_h\n</code></pre>"},{"location":"api/rbm/#pyrkm.rbm.RBM.Deterministic_h_to_v","title":"<code>Deterministic_h_to_v(h, beta)</code>","text":"<p>Deterministically converts hidden units to visible units.</p> <p>Parameters:</p> <ul> <li> <code>h</code>               (<code>Tensor</code>)           \u2013            <p>The hidden units.</p> </li> <li> <code>beta</code>               (<code>float</code>)           \u2013            <p>The inverse temperature parameter.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple</code>           \u2013            <p>The deterministic visible units.</p> </li> </ul> Source code in <code>src/pyrkm/rbm.py</code> <pre><code>def Deterministic_h_to_v(self, h, beta):\n    \"\"\"Deterministically converts hidden units to visible units.\n\n    Parameters\n    ----------\n    h : torch.Tensor\n        The hidden units.\n    beta : float\n        The inverse temperature parameter.\n\n    Returns\n    -------\n    tuple\n        The deterministic visible units.\n    \"\"\"\n    v = (self.delta_ev(h) &gt; 0).to(h.dtype)\n    return v, v\n</code></pre>"},{"location":"api/rbm/#pyrkm.rbm.RBM.Deterministic_v_to_h","title":"<code>Deterministic_v_to_h(v, beta)</code>","text":"<p>Deterministically converts visible units to hidden units.</p> <p>Parameters:</p> <ul> <li> <code>v</code>               (<code>Tensor</code>)           \u2013            <p>The visible units.</p> </li> <li> <code>beta</code>               (<code>float</code>)           \u2013            <p>The inverse temperature parameter.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple</code>           \u2013            <p>The deterministic hidden units.</p> </li> </ul> Source code in <code>src/pyrkm/rbm.py</code> <pre><code>def Deterministic_v_to_h(self, v, beta):\n    \"\"\"Deterministically converts visible units to hidden units.\n\n    Parameters\n    ----------\n    v : torch.Tensor\n        The visible units.\n    beta : float\n        The inverse temperature parameter.\n\n    Returns\n    -------\n    tuple\n        The deterministic hidden units.\n    \"\"\"\n    h = (self.delta_eh(v) &gt; 0).to(v.dtype)\n    return h, h\n</code></pre>"},{"location":"api/rbm/#pyrkm.rbm.RBM.SGD_update","title":"<code>SGD_update(dEdW_data, dEdW_model, dEdv_bias_data, dEdv_bias_model, dEdh_bias_data, dEdh_bias_model)</code>","text":"<p>Updates the model parameters using Stochastic Gradient Descent (SGD).</p> <p>Parameters:</p> <ul> <li> <code>dEdW_data</code>               (<code>Tensor</code>)           \u2013            <p>The gradient of the energy with respect to the weights from the data.</p> </li> <li> <code>dEdW_model</code>               (<code>Tensor</code>)           \u2013            <p>The gradient of the energy with respect to the weights from the model.</p> </li> <li> <code>dEdv_bias_data</code>               (<code>Tensor</code>)           \u2013            <p>The gradient of the energy with respect to the visible biases from the data.</p> </li> <li> <code>dEdv_bias_model</code>               (<code>Tensor</code>)           \u2013            <p>The gradient of the energy with respect to the visible biases from the model.</p> </li> <li> <code>dEdh_bias_data</code>               (<code>Tensor</code>)           \u2013            <p>The gradient of the energy with respect to the hidden biases from the data.</p> </li> <li> <code>dEdh_bias_model</code>               (<code>Tensor</code>)           \u2013            <p>The gradient of the energy with respect to the hidden biases from the model.</p> </li> </ul> Source code in <code>src/pyrkm/rbm.py</code> <pre><code>def SGD_update(self, dEdW_data, dEdW_model, dEdv_bias_data,\n               dEdv_bias_model, dEdh_bias_data, dEdh_bias_model):\n    \"\"\"Updates the model parameters using Stochastic Gradient Descent (SGD).\n\n    Parameters\n    ----------\n    dEdW_data : torch.Tensor\n        The gradient of the energy with respect to the weights from the data.\n    dEdW_model : torch.Tensor\n        The gradient of the energy with respect to the weights from the model.\n    dEdv_bias_data : torch.Tensor\n        The gradient of the energy with respect to the visible biases from the data.\n    dEdv_bias_model : torch.Tensor\n        The gradient of the energy with respect to the visible biases from the model.\n    dEdh_bias_data : torch.Tensor\n        The gradient of the energy with respect to the hidden biases from the data.\n    dEdh_bias_model : torch.Tensor\n        The gradient of the energy with respect to the hidden biases from the model.\n    \"\"\"\n    dW = -dEdW_data + dEdW_model\n    dv = -dEdv_bias_data + dEdv_bias_model\n    dh = -dEdh_bias_data + dEdh_bias_model\n    if self.centering:\n        dv = dv - torch.matmul(self.oh, dW)\n        dh = dh - torch.matmul(self.ov, dW.t())\n    if self.regularization == 'l2':\n        dW -= self.l2 * 2 * self.W\n        dv -= self.l2 * 2 * self.v_bias\n        dh -= self.l2 * 2 * self.h_bias\n    elif self.regularization == 'l1':\n        dW -= self.l1 * torch.sign(self.W)\n        dv -= self.l1 * torch.sign(self.v_bias)\n        dh -= self.l1 * torch.sign(self.h_bias)\n    self.W.add_(self.lr * dW)\n    self.v_bias.add_(self.lr * dv)\n    self.h_bias.add_(self.lr * dh)\n</code></pre>"},{"location":"api/rbm/#pyrkm.rbm.RBM.after_step_keepup","title":"<code>after_step_keepup()</code>","text":"<p>Performs operations to keep the model parameters within specified bounds after each training step.</p> Source code in <code>src/pyrkm/rbm.py</code> <pre><code>def after_step_keepup(self):\n    \"\"\"Performs operations to keep the model parameters\n    within specified bounds after each training step.\"\"\"\n    self.clip_weights()\n    self.clip_bias()\n</code></pre>"},{"location":"api/rbm/#pyrkm.rbm.RBM.av_power_backward","title":"<code>av_power_backward(h)</code>","text":"<p>Computes the average backward power of the hidden units.</p> <p>Parameters:</p> <ul> <li> <code>h</code>               (<code>Tensor</code>)           \u2013            <p>The hidden units.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>The average backward power of the hidden units.</p> </li> </ul> Source code in <code>src/pyrkm/rbm.py</code> <pre><code>def av_power_backward(self, h):\n    \"\"\"Computes the average backward power of the hidden units.\n\n    Parameters\n    ----------\n    h : torch.Tensor\n        The hidden units.\n\n    Returns\n    -------\n    torch.Tensor\n        The average backward power of the hidden units.\n    \"\"\"\n    return self.power_backward(h).mean()\n</code></pre>"},{"location":"api/rbm/#pyrkm.rbm.RBM.av_power_forward","title":"<code>av_power_forward(v)</code>","text":"<p>Computes the average forward power of the visible units.</p> <p>Parameters:</p> <ul> <li> <code>v</code>               (<code>Tensor</code>)           \u2013            <p>The visible units.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>The average forward power of the visible units.</p> </li> </ul> Source code in <code>src/pyrkm/rbm.py</code> <pre><code>def av_power_forward(self, v):\n    \"\"\"Computes the average forward power of the visible units.\n\n    Parameters\n    ----------\n    v : torch.Tensor\n        The visible units.\n\n    Returns\n    -------\n    torch.Tensor\n        The average forward power of the visible units.\n    \"\"\"\n    return self.power_forward(v).mean()\n</code></pre>"},{"location":"api/rbm/#pyrkm.rbm.RBM.clip_bias","title":"<code>clip_bias()</code>","text":"<p>Clips the biases of the RBM model to be within specified bounds.</p> Source code in <code>src/pyrkm/rbm.py</code> <pre><code>def clip_bias(self):\n    \"\"\"Clips the biases of the RBM model to be within specified bounds.\"\"\"\n    self.v_bias = torch.clip(self.v_bias, self.min_W, self.max_W)\n    self.h_bias = torch.clip(self.h_bias, self.min_W, self.max_W)\n</code></pre>"},{"location":"api/rbm/#pyrkm.rbm.RBM.clip_weights","title":"<code>clip_weights()</code>","text":"<p>Clips the weights of the RBM model to be within specified bounds.</p> Source code in <code>src/pyrkm/rbm.py</code> <pre><code>def clip_weights(self):\n    \"\"\"Clips the weights of the RBM model to be within specified bounds.\"\"\"\n    self.W = torch.clip(self.W, self.min_W, self.max_W)\n    self.W_t = self.W.t()\n</code></pre>"},{"location":"api/rbm/#pyrkm.rbm.RBM.delta_eh","title":"<code>delta_eh(v)</code>","text":"<p>Computes the change in energy with respect to the hidden units.</p> <p>Parameters:</p> <ul> <li> <code>v</code>               (<code>Tensor</code>)           \u2013            <p>The visible units.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>The change in energy with respect to the hidden units.</p> </li> </ul> Source code in <code>src/pyrkm/rbm.py</code> <pre><code>def delta_eh(self, v):\n    \"\"\"Computes the change in energy with respect to the hidden units.\n\n    Parameters\n    ----------\n    v : torch.Tensor\n        The visible units.\n\n    Returns\n    -------\n    torch.Tensor\n        The change in energy with respect to the hidden units.\n    \"\"\"\n    return self._delta_eh_hopfield(v)\n</code></pre>"},{"location":"api/rbm/#pyrkm.rbm.RBM.delta_ev","title":"<code>delta_ev(h)</code>","text":"<p>Computes the change in energy with respect to the visible units.</p> <p>Parameters:</p> <ul> <li> <code>h</code>               (<code>Tensor</code>)           \u2013            <p>The hidden units.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>The change in energy with respect to the visible units.</p> </li> </ul> Source code in <code>src/pyrkm/rbm.py</code> <pre><code>def delta_ev(self, h):\n    \"\"\"Computes the change in energy with respect to the visible units.\n\n    Parameters\n    ----------\n    h : torch.Tensor\n        The hidden units.\n\n    Returns\n    -------\n    torch.Tensor\n        The change in energy with respect to the visible units.\n    \"\"\"\n    return self._delta_ev_hopfield(h)\n</code></pre>"},{"location":"api/rbm/#pyrkm.rbm.RBM.derivatives","title":"<code>derivatives(v, h)</code>","text":"<p>Computes the derivatives of the energy with respect to the weights and biases.</p> <p>Parameters:</p> <ul> <li> <code>v</code>               (<code>Tensor</code>)           \u2013            <p>The visible units.</p> </li> <li> <code>h</code>               (<code>Tensor</code>)           \u2013            <p>The hidden units.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple</code>           \u2013            <p>The derivatives of the energy with respect to the weights, visible biases, and hidden biases.</p> </li> </ul> Source code in <code>src/pyrkm/rbm.py</code> <pre><code>def derivatives(self, v, h):\n    \"\"\"Computes the derivatives of the energy with respect to the weights and biases.\n\n    Parameters\n    ----------\n    v : torch.Tensor\n        The visible units.\n    h : torch.Tensor\n        The hidden units.\n\n    Returns\n    -------\n    tuple\n        The derivatives of the energy with respect to the weights, visible biases, and hidden biases.\n    \"\"\"\n    return self.derivatives_hopfield(v, h)\n</code></pre>"},{"location":"api/rbm/#pyrkm.rbm.RBM.derivatives_hopfield","title":"<code>derivatives_hopfield(v, h)</code>","text":"<p>Computes the derivatives of the energy with respect to the weights and biases using the Hopfield energy function.</p> <p>Parameters:</p> <ul> <li> <code>v</code>               (<code>Tensor</code>)           \u2013            <p>The visible units.</p> </li> <li> <code>h</code>               (<code>Tensor</code>)           \u2013            <p>The hidden units.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple</code>           \u2013            <p>The derivatives of the energy with respect to the weights, visible biases, and hidden biases.</p> </li> </ul> Source code in <code>src/pyrkm/rbm.py</code> <pre><code>def derivatives_hopfield(self, v, h):\n    \"\"\"Computes the derivatives of the energy with respect to\n    the weights and biases using the Hopfield energy function.\n\n    Parameters\n    ----------\n    v : torch.Tensor\n        The visible units.\n    h : torch.Tensor\n        The hidden units.\n\n    Returns\n    -------\n    tuple\n        The derivatives of the energy with respect to the weights, visible biases, and hidden biases.\n    \"\"\"\n    if self.centering:\n        dEdW = -torch.einsum('ij,ik-&gt;ijk', h - self.oh, v - self.ov)\n    else:\n        dEdW = -torch.einsum('ij,ik-&gt;ijk', h, v)\n    dEdv_bias = -v\n    dEdh_bias = -h\n    return dEdW, dEdv_bias, dEdh_bias\n</code></pre>"},{"location":"api/rbm/#pyrkm.rbm.RBM.forward","title":"<code>forward(v, k, beta=None)</code>","text":"<p>Performs a forward pass through the RBM model.</p> <p>Parameters:</p> <ul> <li> <code>v</code>               (<code>Tensor</code>)           \u2013            <p>The visible units.</p> </li> <li> <code>k</code>               (<code>int</code>)           \u2013            <p>The number of Gibbs sampling steps.</p> </li> <li> <code>beta</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>The inverse temperature parameter (default is None).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>The reconstructed visible units.</p> </li> </ul> Source code in <code>src/pyrkm/rbm.py</code> <pre><code>def forward(self, v, k, beta=None):\n    \"\"\"Performs a forward pass through the RBM model.\n\n    Parameters\n    ----------\n    v : torch.Tensor\n        The visible units.\n    k : int\n        The number of Gibbs sampling steps.\n    beta : float, optional\n        The inverse temperature parameter (default is None).\n\n    Returns\n    -------\n    torch.Tensor\n        The reconstructed visible units.\n    \"\"\"\n    if beta is None:\n        beta = self.model_beta\n    pre_h1, h1 = self.v_to_h(v, beta)\n    h_ = h1\n    for _ in range(k):\n        pre_v_, v_ = self.h_to_v(h_, beta)\n        pre_h_, h_ = self.v_to_h(v_, beta)\n    return v_\n</code></pre>"},{"location":"api/rbm/#pyrkm.rbm.RBM.free_energy","title":"<code>free_energy(v, beta=None)</code>","text":"<p>Computes the free energy of the visible units.</p> <p>Parameters:</p> <ul> <li> <code>v</code>               (<code>Tensor</code>)           \u2013            <p>The visible units.</p> </li> <li> <code>beta</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>The inverse temperature parameter (default is None).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>The free energy of the visible units.</p> </li> </ul> Source code in <code>src/pyrkm/rbm.py</code> <pre><code>def free_energy(self, v, beta=None):\n    \"\"\"Computes the free energy of the visible units.\n\n    Parameters\n    ----------\n    v : torch.Tensor\n        The visible units.\n    beta : float, optional\n        The inverse temperature parameter (default is None).\n\n    Returns\n    -------\n    torch.Tensor\n        The free energy of the visible units.\n    \"\"\"\n    if beta is None:\n        beta = self.model_beta\n    return self._free_energy_hopfield(v, beta)\n</code></pre>"},{"location":"api/rbm/#pyrkm.rbm.RBM.generate","title":"<code>generate(n_samples, k, h_binarized=True, from_visible=True, beta=None)</code>","text":"<p>Generates samples from the RBM model.</p> <p>Parameters:</p> <ul> <li> <code>n_samples</code>               (<code>int</code>)           \u2013            <p>The number of samples to generate.</p> </li> <li> <code>k</code>               (<code>int</code>)           \u2013            <p>The number of Gibbs sampling steps.</p> </li> <li> <code>h_binarized</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to binarize the hidden units (default is True).</p> </li> <li> <code>from_visible</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to generate samples from visible units (default is True).</p> </li> <li> <code>beta</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>The inverse temperature parameter (default is None).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The generated samples.</p> </li> </ul> Source code in <code>src/pyrkm/rbm.py</code> <pre><code>def generate(self,\n             n_samples,\n             k,\n             h_binarized=True,\n             from_visible=True,\n             beta=None):\n    \"\"\"Generates samples from the RBM model.\n\n    Parameters\n    ----------\n    n_samples : int\n        The number of samples to generate.\n    k : int\n        The number of Gibbs sampling steps.\n    h_binarized : bool, optional\n        Whether to binarize the hidden units (default is True).\n    from_visible : bool, optional\n        Whether to generate samples from visible units (default is True).\n    beta : float, optional\n        The inverse temperature parameter (default is None).\n\n    Returns\n    -------\n    numpy.ndarray\n        The generated samples.\n    \"\"\"\n    if beta is None:\n        beta = self.model_beta\n    if from_visible:\n        v = torch.randint(high=2,\n                          size=(n_samples, self.n_visible),\n                          device=self.device,\n                          dtype=self.mytype)\n    else:\n        if h_binarized:\n            h = torch.randint(high=2,\n                              size=(n_samples, self.n_hidden),\n                              device=self.device,\n                              dtype=self.mytype)\n        else:\n            h = torch.rand(n_samples,\n                           self.n_hidden,\n                           device=self.device,\n                           dtype=self.mytype)\n        _, v = self.h_to_v(h)\n    v_model = self.forward(v, k, beta)\n    return v_model.detach().cpu().numpy()\n</code></pre>"},{"location":"api/rbm/#pyrkm.rbm.RBM.h_to_v","title":"<code>h_to_v(h, beta=None)</code>","text":"<p>Converts hidden units to visible units.</p> <p>Parameters:</p> <ul> <li> <code>h</code>               (<code>Tensor</code>)           \u2013            <p>The hidden units.</p> </li> <li> <code>beta</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>The inverse temperature parameter (default is None).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple</code>           \u2013            <p>The probabilities and samples of the visible units.</p> </li> </ul> Source code in <code>src/pyrkm/rbm.py</code> <pre><code>def h_to_v(self, h, beta=None):\n    \"\"\"Converts hidden units to visible units.\n\n    Parameters\n    ----------\n    h : torch.Tensor\n        The hidden units.\n    beta : float, optional\n        The inverse temperature parameter (default is None).\n\n    Returns\n    -------\n    tuple\n        The probabilities and samples of the visible units.\n    \"\"\"\n    if beta is None:\n        beta = self.model_beta\n    else:\n        if beta &gt; 1000:\n            return self.Deterministic_h_to_v(h, beta)\n    return self.Bernoulli_h_to_v(h, beta)\n</code></pre>"},{"location":"api/rbm/#pyrkm.rbm.RBM.plot_bias","title":"<code>plot_bias(t)</code>","text":"<p>Plots the hidden and visible biases of the RBM model.</p> <p>Parameters:</p> <ul> <li> <code>t</code>               (<code>int</code>)           \u2013            <p>The current epoch.</p> </li> </ul> Source code in <code>src/pyrkm/rbm.py</code> <pre><code>def plot_bias(self, t):\n    \"\"\"Plots the hidden and visible biases of the RBM model.\n\n    Parameters\n    ----------\n    t : int\n        The current epoch.\n    \"\"\"\n    h_bias = self.h_bias.detach().cpu().numpy()\n    v_bias = self.v_bias.detach().cpu().numpy()\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n    ax1.hist(h_bias, bins=20, color='blue', edgecolor='black')\n    ax1.set_xlabel('Values')\n    ax1.set_ylabel('Frequency')\n    ax1.set_title('Hidden Biases epoch {}'.format(t))\n    ax2.hist(v_bias, bins=20, color='red', edgecolor='black')\n    ax2.set_xlabel('Values')\n    ax2.set_ylabel('Frequency')\n    ax2.set_title('Visible Biases epoch {}'.format(t))\n    plt.tight_layout()\n</code></pre>"},{"location":"api/rbm/#pyrkm.rbm.RBM.plot_visible_bias","title":"<code>plot_visible_bias(t)</code>","text":"<p>Plots the visible biases of the RBM model.</p> <p>Parameters:</p> <ul> <li> <code>t</code>               (<code>int</code>)           \u2013            <p>The current epoch.</p> </li> </ul> Source code in <code>src/pyrkm/rbm.py</code> <pre><code>def plot_visible_bias(self, t):\n    \"\"\"Plots the visible biases of the RBM model.\n\n    Parameters\n    ----------\n    t : int\n        The current epoch.\n    \"\"\"\n    data_2d = self.v_bias.detach().cpu().numpy().reshape(28, 28)\n    fig, ax = plt.subplots(figsize=(5, 5))\n    im = ax.imshow(data_2d, cmap='magma')\n    cbar = ax.figure.colorbar(im, ax=ax)\n    cbar.ax.set_ylabel('Values', rotation=-90, va='bottom')\n    ax.set_title('Visible Biases epoch {}'.format(t))\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Rows')\n</code></pre>"},{"location":"api/rbm/#pyrkm.rbm.RBM.plot_weights","title":"<code>plot_weights(t)</code>","text":"<p>Plots the weights of the RBM model.</p> <p>Parameters:</p> <ul> <li> <code>t</code>               (<code>int</code>)           \u2013            <p>The current epoch.</p> </li> </ul> Source code in <code>src/pyrkm/rbm.py</code> <pre><code>def plot_weights(self, t):\n    \"\"\"Plots the weights of the RBM model.\n\n    Parameters\n    ----------\n    t : int\n        The current epoch.\n    \"\"\"\n    Ndata = self.W.shape[0]\n    data_3d = self.W.detach().cpu().numpy().reshape(Ndata, 28, 28)\n    num_rows = int(np.ceil(np.sqrt(Ndata)))\n    num_cols = int(np.ceil(Ndata / num_rows))\n    fig, ax = plt.subplots(nrows=num_rows,\n                           ncols=num_cols,\n                           figsize=(10, 10))\n    for i in range(Ndata):\n        row = i // num_cols\n        col = i % num_cols\n        ax[row, col].imshow(data_3d[i], cmap='magma')\n        ax[row, col].axis('off')\n    if num_rows * num_cols &gt; Ndata:\n        for i in range(Ndata, num_rows * num_cols):\n            row = i // num_cols\n            col = i % num_cols\n            fig.delaxes(ax[row, col])\n    plt.suptitle('Weights epoch {}'.format(t))\n    plt.subplots_adjust(wspace=0.05, hspace=0.05, top=0.9)\n    vmin = np.min(self.W.detach().cpu().numpy())\n    vmax = np.max(self.W.detach().cpu().numpy())\n    dummy_img = np.zeros((1, 1))\n    cax = fig.add_axes([0.93, 0.15, 0.02, 0.7])\n    plt.colorbar(plt.imshow(dummy_img, cmap='magma', vmin=vmin, vmax=vmax),\n                 cax=cax)\n    cax.set_aspect('auto')\n</code></pre>"},{"location":"api/rbm/#pyrkm.rbm.RBM.power_backward","title":"<code>power_backward(h)</code>","text":"<p>Computes the backward power of the hidden units.</p> <p>Parameters:</p> <ul> <li> <code>h</code>               (<code>Tensor</code>)           \u2013            <p>The hidden units.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>The backward power of the hidden units.</p> </li> </ul> Source code in <code>src/pyrkm/rbm.py</code> <pre><code>def power_backward(self, h):\n    \"\"\"Computes the backward power of the hidden units.\n\n    Parameters\n    ----------\n    h : torch.Tensor\n        The hidden units.\n\n    Returns\n    -------\n    torch.Tensor\n        The backward power of the hidden units.\n    \"\"\"\n    h_centered = h - 0.5\n    W_centered, v_bias_centered, h_bias_centered = self._center()\n    v_eq = self._RKM_h_to_v(h_centered, W_centered, v_bias_centered,\n                            h_bias_centered)\n\n    power = (torch.matmul(h_centered**2,\n                          torch.abs(W_centered / 2).sum(dim=0)) +\n             torch.matmul(v_eq**2,\n                          torch.abs(W_centered / 2).sum(dim=1)) -\n             torch.einsum('ij,ji-&gt;i', h_centered,\n                          torch.matmul(W_centered.T, v_eq.T)) +\n             torch.matmul(\n                 (v_eq**2 + self.g_v**2), torch.abs(v_bias_centered)) -\n             torch.matmul(v_eq, v_bias_centered) * self.g_v)\n\n    return power\n</code></pre>"},{"location":"api/rbm/#pyrkm.rbm.RBM.power_forward","title":"<code>power_forward(v)</code>","text":"<p>Computes the forward power of the visible units.</p> <p>Parameters:</p> <ul> <li> <code>v</code>               (<code>Tensor</code>)           \u2013            <p>The visible units.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>The forward power of the visible units.</p> </li> </ul> Source code in <code>src/pyrkm/rbm.py</code> <pre><code>def power_forward(self, v):\n    \"\"\"Computes the forward power of the visible units.\n\n    Parameters\n    ----------\n    v : torch.Tensor\n        The visible units.\n\n    Returns\n    -------\n    torch.Tensor\n        The forward power of the visible units.\n    \"\"\"\n    v_centered = v - 0.5\n    W_centered, v_bias_centered, h_bias_centered = self._center()\n    h_eq = self._RKM_v_to_h(v_centered, W_centered, v_bias_centered,\n                            h_bias_centered)\n\n    power = (torch.matmul(v_centered**2,\n                          torch.abs(W_centered / 2).sum(dim=1)) +\n             torch.matmul(h_eq**2,\n                          torch.abs(W_centered / 2).sum(dim=0)) -\n             torch.einsum('ij,ji-&gt;i', v_centered,\n                          torch.matmul(W_centered, h_eq.T)) +\n             torch.matmul(\n                 (h_eq**2 + self.g_h**2), torch.abs(h_bias_centered)) -\n             torch.matmul(h_eq, h_bias_centered) * self.g_h)\n\n    return power\n</code></pre>"},{"location":"api/rbm/#pyrkm.rbm.RBM.pretrain","title":"<code>pretrain(pretrained_model, model_state_path='model_states/')</code>","text":"<p>Loads pretrained parameters from a specified model.</p> <p>Parameters:</p> <ul> <li> <code>pretrained_model</code>               (<code>str</code>)           \u2013            <p>The name of the pretrained model.</p> </li> <li> <code>model_state_path</code>               (<code>str</code>, default:                   <code>'model_states/'</code> )           \u2013            <p>The path to the directory containing the model states (default is 'model_states/').</p> </li> </ul> Source code in <code>src/pyrkm/rbm.py</code> <pre><code>def pretrain(self, pretrained_model, model_state_path='model_states/'):\n    \"\"\"Loads pretrained parameters from a specified model.\n\n    Parameters\n    ----------\n    pretrained_model : str\n        The name of the pretrained model.\n    model_state_path : str, optional\n        The path to the directory containing the model states (default is 'model_states/').\n    \"\"\"\n    ensure_dir(model_state_path)\n    filename_list = glob.glob(model_state_path +\n                              '{}_t*.pkl'.format(pretrained_model))\n    if len(filename_list) &gt; 0:\n        all_loadpoints = sorted([\n            int(x.split('_t')[-1].split('.pkl')[0]) for x in filename_list\n        ])\n        last_epoch = all_loadpoints[-1]\n        print('** Using as pretraining model {} at epoch {}'.format(\n            pretrained_model, last_epoch),\n              flush=True)\n        with open(\n                model_state_path +\n                '{}_t{}.pkl'.format(pretrained_model, last_epoch),\n                # *** Import pretrained parameters\n                'rb') as file:\n            temp_model = pickle.load(file)\n            # *** Import pretrained parameters\n            self.W = temp_model.W.to(self.mytype)\n            self.h_bias = temp_model.h_bias.to(self.mytype)\n            self.v_bias = temp_model.v_bias.to(self.mytype)\n    else:\n        print('** No load points for {}'.format(pretrained_model),\n              flush=True)\n</code></pre>"},{"location":"api/rbm/#pyrkm.rbm.RBM.reconstruct","title":"<code>reconstruct(data, k)</code>","text":"<p>Reconstructs the visible units from the data using k Gibbs sampling steps.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>array - like</code>)           \u2013            <p>The input data.</p> </li> <li> <code>k</code>               (<code>int</code>)           \u2013            <p>The number of Gibbs sampling steps.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple</code>           \u2013            <p>The original and reconstructed visible units.</p> </li> </ul> Source code in <code>src/pyrkm/rbm.py</code> <pre><code>def reconstruct(self, data, k):\n    \"\"\"Reconstructs the visible units from the data using k Gibbs sampling steps.\n\n    Parameters\n    ----------\n    data : array-like\n        The input data.\n    k : int\n        The number of Gibbs sampling steps.\n\n    Returns\n    -------\n    tuple\n        The original and reconstructed visible units.\n    \"\"\"\n    data = torch.Tensor(data).to(self.device).to(self.mytype)\n    v_model = self.forward(data, k)\n    return data.detach().cpu().numpy(), v_model.detach().cpu().numpy()\n</code></pre>"},{"location":"api/rbm/#pyrkm.rbm.RBM.relaxation_times","title":"<code>relaxation_times()</code>","text":"<p>Computes the relaxation times for the forward and backward passes.</p> <p>Returns:</p> <ul> <li> <code>tuple</code>           \u2013            <p>The relaxation times for the forward and backward passes.</p> </li> </ul> Source code in <code>src/pyrkm/rbm.py</code> <pre><code>def relaxation_times(self):\n    \"\"\"Computes the relaxation times for the forward and backward passes.\n\n    Returns\n    -------\n    tuple\n        The relaxation times for the forward and backward passes.\n    \"\"\"\n    W_centered, v_bias_centered, h_bias_centered = self._center()\n    t_forward = 1 / (torch.abs(W_centered / 2).sum(dim=0) +\n                     torch.abs(h_bias_centered))\n    t_backward = 1 / (torch.abs(W_centered / 2).sum(dim=1) +\n                      torch.abs(v_bias_centered))\n\n    return t_forward, t_backward\n</code></pre>"},{"location":"api/rbm/#pyrkm.rbm.RBM.train","title":"<code>train(train_data, test_data=[], print_error=False, print_test_error=False, model_state_path='model_states/', print_every=100)</code>","text":"<p>Trains the RBM model using the specified training algorithm.</p> <p>Parameters:</p> <ul> <li> <code>train_data</code>               (<code>iterable</code>)           \u2013            <p>The training data.</p> </li> <li> <code>test_data</code>               (<code>iterable</code>, default:                   <code>[]</code> )           \u2013            <p>The test data (default is an empty list).</p> </li> <li> <code>print_error</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to print the training error (default is False).</p> </li> <li> <code>print_test_error</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to print the test error (default is False).</p> </li> <li> <code>model_state_path</code>               (<code>str</code>, default:                   <code>'model_states/'</code> )           \u2013            <p>The path to the directory containing the model states (default is 'model_states/').</p> </li> <li> <code>print_every</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The number of epochs between printing the training status (default is 100).</p> </li> </ul> Source code in <code>src/pyrkm/rbm.py</code> <pre><code>def train(self,\n          train_data,\n          test_data=[],\n          print_error=False,\n          print_test_error=False,\n          model_state_path='model_states/',\n          print_every=100):\n    \"\"\"Trains the RBM model using the specified training algorithm.\n\n    Parameters\n    ----------\n    train_data : iterable\n        The training data.\n    test_data : iterable, optional\n        The test data (default is an empty list).\n    print_error : bool, optional\n        Whether to print the training error (default is False).\n    print_test_error : bool, optional\n        Whether to print the test error (default is False).\n    model_state_path : str, optional\n        The path to the directory containing the model states (default is 'model_states/').\n    print_every : int, optional\n        The number of epochs between printing the training status (default is 100).\n    \"\"\"\n    while self.epoch &lt; self.max_epochs:\n        self.W_t = self.W.t()\n\n        for _, v_data in enumerate(train_data):\n\n            start_time = time.time()\n            self.power_f = 0\n            self.power_b = 0\n\n            h_data = self.v_to_h(v_data)[1]\n            p_f = self.power_forward(v_data)\n            self.power_f += p_f.mean()\n            self.energy += p_f.sum()\n\n            if self.train_algo == 'PCD':\n                v_model = self.persistent_chains\n                for _ in range(self.k):\n                    h_model = self.v_to_h(v_model)[1]\n                    p_f = self.power_forward(v_model)\n                    self.power_f += p_f.mean()\n\n                    v_model = self.h_to_v(h_model)[1]\n                    p_b = self.power_backward(h_model)\n                    self.power_b += p_b.mean()\n\n                    self.energy += p_f.sum() + p_b.sum()\n\n                self.persistent_chains = v_model\n\n            elif self.train_algo == 'RDM':\n                v_model = torch.randint(high=2,\n                                        size=(self.batch_size,\n                                              self.n_visible),\n                                        device=self.device,\n                                        dtype=self.mytype)\n                v_model = self.forward(v_model, self.k)\n                print(\n                    'Warning: No physical measurements are implemented for RDM training algorithm.'\n                    + 'Use hRDM or vRDM instead.')\n            elif self.train_algo == 'CD':\n                v_model = v_data\n                for _ in range(self.k):\n                    h_model = self.v_to_h(v_model, self.model_beta)[1]\n                    p_f = self.power_forward(v_model)\n                    self.power_f += p_f.mean()\n\n                    v_model = self.h_to_v(h_model, self.model_beta)[1]\n                    p_b = self.power_backward(h_model)\n                    self.power_b += p_b.mean()\n\n                    self.energy += p_f.sum() + p_b.sum()\n            elif self.train_algo == 'vRDM':\n                v_model = torch.randint(high=2,\n                                        size=(self.batch_size,\n                                              self.n_visible),\n                                        device=self.device,\n                                        dtype=self.mytype)\n                for _ in range(self.k):\n                    h_model = self.v_to_h(v_model, self.model_beta)[1]\n                    p_f = self.power_forward(v_model)\n                    self.power_f += p_f.mean()\n\n                    v_model = self.h_to_v(h_model, self.model_beta)[1]\n                    p_b = self.power_backward(h_model)\n                    self.power_b += p_b.mean()\n\n                    self.energy += p_f.sum() + p_b.sum()\n            elif self.train_algo == 'hRDM':\n                h_model = torch.randint(high=2,\n                                        size=(self.batch_size,\n                                              self.n_hidden),\n                                        device=self.device,\n                                        dtype=self.mytype)\n                v_model = self.h_to_v(h_model, self.model_beta)[1]\n                p_b = self.power_backward(h_model)\n                self.power_b += p_b.mean()\n\n                self.energy += p_b.sum()\n\n                for _ in range(self.k - 1):\n                    h_model = self.v_to_h(v_model, self.model_beta)[1]\n                    p_f = self.power_forward(v_model)\n                    self.power_f += p_f.mean()\n\n                    v_model = self.h_to_v(h_model, self.model_beta)[1]\n                    p_b = self.power_backward(h_model)\n                    self.power_b += p_b.mean()\n\n                    self.energy += p_f.sum() + p_b.sum()\n\n            if self.centering:\n                self.batch_ov = v_data.mean(0)\n                self.batch_oh = h_data.mean(0)\n                self.ov = (1 -\n                           self.slv) * self.ov + self.slv * self.batch_ov\n                self.oh = (1 -\n                           self.slh) * self.oh + self.slh * self.batch_oh\n\n            dEdW_data, dEdv_bias_data, dEdh_bias_data = self.derivatives(\n                v_data, h_data)\n            dEdW_model, dEdv_bias_model, dEdh_bias_model = self.derivatives(\n                v_model, h_model)\n\n            dEdW_data = torch.mean(dEdW_data, dim=0)\n            dEdv_bias_data = torch.mean(dEdv_bias_data, dim=0)\n            dEdh_bias_data = torch.mean(dEdh_bias_data, dim=0)\n            dEdW_model = torch.mean(dEdW_model, dim=0)\n            dEdv_bias_model = torch.mean(dEdv_bias_model, dim=0)\n            dEdh_bias_model = torch.mean(dEdh_bias_model, dim=0)\n\n            if self.optimizer == 'Adam':\n                self.Adam_update(self.epoch + 1, dEdW_data, dEdW_model,\n                                 dEdv_bias_data, dEdv_bias_model,\n                                 dEdh_bias_data, dEdh_bias_model)\n            elif self.optimizer == 'SGD':\n                self.SGD_update(dEdW_data, dEdW_model, dEdv_bias_data,\n                                dEdv_bias_model, dEdh_bias_data,\n                                dEdh_bias_model)\n\n            self.after_step_keepup()\n\n            self.relax_t_f, self.relax_t_b = self.relaxation_times()\n\n            self.epoch += 1\n\n            if self.epoch in self.t_to_save:\n                ensure_dir(model_state_path)\n                with open(\n                        model_state_path +\n                        '{}_t{}.pkl'.format(self.model_name, self.epoch),\n                        'wb') as file:\n                    pickle.dump(self, file)\n\n            if self.epoch % print_every == 0:\n                t = time.time() - start_time\n                if print_error:\n                    v_model = self.forward(v_data, 1)\n                    rec_error_train = ((v_model -\n                                        v_data)**2).mean(1).mean(0)\n                    if not print_test_error:\n                        print('Epoch: %d , train-err %.5g , time: %f' %\n                              (self.epoch, rec_error_train, t),\n                              flush=True)\n                    else:\n                        t_model = self.forward(test_data, 1)\n                        rec_error_test = ((t_model -\n                                           test_data)**2).mean(1).mean(0)\n                        print(\n                            'Epoch: %d , Test-err %.5g , train-err %.5g , time: %f'\n                            % (self.epoch, rec_error_test, rec_error_train,\n                               t),\n                            flush=True)\n                else:\n                    print('Epoch: %d , time: %f' % (self.epoch, t),\n                          flush=True)\n\n    print('*** Training finished', flush=True)\n</code></pre>"},{"location":"api/rbm/#pyrkm.rbm.RBM.v_to_h","title":"<code>v_to_h(v, beta=None)</code>","text":"<p>Converts visible units to hidden units.</p> <p>Parameters:</p> <ul> <li> <code>v</code>               (<code>Tensor</code>)           \u2013            <p>The visible units.</p> </li> <li> <code>beta</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>The inverse temperature parameter (default is None).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple</code>           \u2013            <p>The probabilities and samples of the hidden units.</p> </li> </ul> Source code in <code>src/pyrkm/rbm.py</code> <pre><code>def v_to_h(self, v, beta=None):\n    \"\"\"Converts visible units to hidden units.\n\n    Parameters\n    ----------\n    v : torch.Tensor\n        The visible units.\n    beta : float, optional\n        The inverse temperature parameter (default is None).\n\n    Returns\n    -------\n    tuple\n        The probabilities and samples of the hidden units.\n    \"\"\"\n    if beta is None:\n        beta = self.model_beta\n    else:\n        if beta &gt; 1000:\n            return self.Deterministic_v_to_h(v, beta)\n    return self.Bernoulli_v_to_h(v, beta)\n</code></pre>"},{"location":"api/rkm/","title":"RKM class","text":""},{"location":"api/rkm/#pyrkm.rkm.RKM","title":"<code>RKM(model_name, n_visible, n_hidden, k=1, lr=0.001, max_epochs=200000, energy_type='RKM', optimizer='SGD', regularization=False, l1_factor=0, l2_factor=0.001, g_v=0.5, g_h=0.5, batch_size=1, train_algo='vRDM', centering=False, average_data=None, model_beta=1, mytype=torch.float32, min_W=-10, max_W=10, offset=0.0, sampling='bernoulli', distribution='gaussian', layer_scaled=True)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RBM</code></p> <p>A class to represent a Restricted Kirchhoff Machine (RKM). It is inherited from the RBM class, so look at the RBM class for info about the attributes and methods. Also, refer to the paper XXX for more details on the RKM.</p> <p>Parameters:</p> <ul> <li> <code>energy_type</code>               (<code>str</code>, default:                   <code>'RKM'</code> )           \u2013            <p>The type of energy function to use (default is 'RKM').</p> </li> <li> <code>offset</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Offset parameter for the energy function (default is 0.0).</p> </li> <li> <code>sampling</code>               (<code>str</code>, default:                   <code>'bernoulli'</code> )           \u2013            <p>Sampling method to use (default is 'bernoulli').</p> </li> <li> <code>distribution</code>               (<code>str</code>, default:                   <code>'gaussian'</code> )           \u2013            <p>Distribution to use for sampling (default is 'gaussian').</p> </li> <li> <code>layer_scaled</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to scale the layer by the number of units (default is True).</p> </li> </ul>"},{"location":"api/rkm/#pyrkm.rkm.RKM.Adam_update","title":"<code>Adam_update(t, dEdW_data, dEdW_model, dEdv_bias_data, dEdv_bias_model, dEdh_bias_data, dEdh_bias_model)</code>","text":"<p>Update the model parameters using Adam optimizer.</p> <p>Parameters:</p> <ul> <li> <code>t</code>               (<code>int</code>)           \u2013            <p>Current time step.</p> </li> <li> <code>dEdW_data</code>               (<code>Tensor</code>)           \u2013            <p>Gradient of the weights from data.</p> </li> <li> <code>dEdW_model</code>               (<code>Tensor</code>)           \u2013            <p>Gradient of the weights from the model.</p> </li> <li> <code>dEdv_bias_data</code>               (<code>Tensor</code>)           \u2013            <p>Gradient of the visible biases from data.</p> </li> <li> <code>dEdv_bias_model</code>               (<code>Tensor</code>)           \u2013            <p>Gradient of the visible biases from the model.</p> </li> <li> <code>dEdh_bias_data</code>               (<code>Tensor</code>)           \u2013            <p>Gradient of the hidden biases from data.</p> </li> <li> <code>dEdh_bias_model</code>               (<code>Tensor</code>)           \u2013            <p>Gradient of the hidden biases from the model.</p> </li> </ul> Source code in <code>src/pyrkm/rkm.py</code> <pre><code>def Adam_update(self, t, dEdW_data, dEdW_model, dEdv_bias_data,\n                dEdv_bias_model, dEdh_bias_data, dEdh_bias_model):\n    \"\"\"Update the model parameters using Adam optimizer.\n\n    Parameters\n    ----------\n    t : int\n        Current time step.\n    dEdW_data : torch.Tensor\n        Gradient of the weights from data.\n    dEdW_model : torch.Tensor\n        Gradient of the weights from the model.\n    dEdv_bias_data : torch.Tensor\n        Gradient of the visible biases from data.\n    dEdv_bias_model : torch.Tensor\n        Gradient of the visible biases from the model.\n    dEdh_bias_data : torch.Tensor\n        Gradient of the hidden biases from data.\n    dEdh_bias_model : torch.Tensor\n        Gradient of the hidden biases from the model.\n    \"\"\"\n    # Gradients\n    dW = -dEdW_data + dEdW_model\n    dv = -dEdv_bias_data + dEdv_bias_model\n    dh = -dEdh_bias_data + dEdh_bias_model\n    if self.centering:\n        dv = dv - torch.matmul(self.oh, dW)\n        dh = dh - torch.matmul(self.ov, dW.t())\n    # Add regularization term\n    if self.regularization == 'l2':\n        dW += self.l2 * 2 * self.W\n        dv += self.l2 * 2 * self.v_bias\n        dh += self.l2 * 2 * self.h_bias\n    elif self.regularization == 'l1':\n        dW += self.l1 * torch.sign(self.W)\n        dv += self.l1 * torch.sign(self.v_bias)\n        dh += self.l1 * torch.sign(self.h_bias)\n    # momentum beta1\n    self.m_dW = self.beta1 * self.m_dW + (1 - self.beta1) * dW\n    self.m_dv = self.beta1 * self.m_dv + (1 - self.beta1) * dv\n    self.m_dh = self.beta1 * self.m_dh + (1 - self.beta1) * dh\n    # momentum beta2\n    self.v_dW = self.beta2 * self.v_dW + (1 - self.beta2) * (dW**2)\n    self.v_dv = self.beta2 * self.v_dv + (1 - self.beta2) * (dv**2)\n    self.v_dh = self.beta2 * self.v_dh + (1 - self.beta2) * (dh**2)\n    # bias correction\n    m_dW_corr = self.m_dW / (1 - self.beta1**t)\n    m_dv_corr = self.m_dv / (1 - self.beta1**t)\n    m_dh_corr = self.m_dh / (1 - self.beta1**t)\n    v_dW_corr = self.v_dW / (1 - self.beta2**t)\n    v_dv_corr = self.v_dv / (1 - self.beta2**t)\n    v_dh_corr = self.v_dh / (1 - self.beta2**t)\n    # Update\n    self.W = self.W + self.lr * (m_dW_corr /\n                                 (torch.sqrt(v_dW_corr) + self.epsilon))\n    self.v_bias = self.v_bias + self.lr * (\n        m_dv_corr / (torch.sqrt(v_dv_corr) + self.epsilon))\n    self.h_bias = self.h_bias + self.lr * (\n        m_dh_corr / (torch.sqrt(v_dh_corr) + self.epsilon))\n</code></pre>"},{"location":"api/rkm/#pyrkm.rkm.RKM.Bernoulli_h_to_v","title":"<code>Bernoulli_h_to_v(h, beta)</code>","text":"<p>Convert hidden units to visible units using Bernoulli sampling.</p> <p>Parameters:</p> <ul> <li> <code>h</code>               (<code>Tensor</code>)           \u2013            <p>Hidden units.</p> </li> <li> <code>beta</code>               (<code>float</code>)           \u2013            <p>Inverse temperature parameter.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple of torch.Tensor</code>           \u2013            <p>Probabilities and sampled visible units.</p> </li> </ul> Source code in <code>src/pyrkm/rkm.py</code> <pre><code>def Bernoulli_h_to_v(self, h, beta):\n    \"\"\"Convert hidden units to visible units using Bernoulli sampling.\n\n    Parameters\n    ----------\n    h : torch.Tensor\n        Hidden units.\n    beta : float\n        Inverse temperature parameter.\n\n    Returns\n    -------\n    tuple of torch.Tensor\n        Probabilities and sampled visible units.\n    \"\"\"\n    p_v = self._prob_v_given_h(h, beta)\n    sample_v = torch.bernoulli(p_v)\n    return p_v, sample_v\n</code></pre>"},{"location":"api/rkm/#pyrkm.rkm.RKM.Bernoulli_v_to_h","title":"<code>Bernoulli_v_to_h(v, beta)</code>","text":"<p>Convert visible units to hidden units using Bernoulli sampling.</p> <p>Parameters:</p> <ul> <li> <code>v</code>               (<code>Tensor</code>)           \u2013            <p>Visible units.</p> </li> <li> <code>beta</code>               (<code>float</code>)           \u2013            <p>Inverse temperature parameter.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple of torch.Tensor</code>           \u2013            <p>Probabilities and sampled hidden units.</p> </li> </ul> Source code in <code>src/pyrkm/rkm.py</code> <pre><code>def Bernoulli_v_to_h(self, v, beta):\n    \"\"\"Convert visible units to hidden units using Bernoulli sampling.\n\n    Parameters\n    ----------\n    v : torch.Tensor\n        Visible units.\n    beta : float\n        Inverse temperature parameter.\n\n    Returns\n    -------\n    tuple of torch.Tensor\n        Probabilities and sampled hidden units.\n    \"\"\"\n    p_h = self._prob_h_given_v(v, beta)\n    sample_h = torch.bernoulli(p_h)\n    return p_h, sample_h\n</code></pre>"},{"location":"api/rkm/#pyrkm.rkm.RKM.Deterministic_h_to_v","title":"<code>Deterministic_h_to_v(h, beta)</code>","text":"<p>Deterministically convert hidden units to visible units.</p> <p>Parameters:</p> <ul> <li> <code>h</code>               (<code>Tensor</code>)           \u2013            <p>Hidden units.</p> </li> <li> <code>beta</code>               (<code>float</code>)           \u2013            <p>Inverse temperature parameter.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple of torch.Tensor</code>           \u2013            <p>Deterministic visible units.</p> </li> </ul> Source code in <code>src/pyrkm/rkm.py</code> <pre><code>def Deterministic_h_to_v(self, h, beta):\n    \"\"\"Deterministically convert hidden units to visible units.\n\n    Parameters\n    ----------\n    h : torch.Tensor\n        Hidden units.\n    beta : float\n        Inverse temperature parameter.\n\n    Returns\n    -------\n    tuple of torch.Tensor\n        Deterministic visible units.\n    \"\"\"\n    v = (self.delta_ev(h) &gt; 0).to(h.dtype)\n    return v, v\n</code></pre>"},{"location":"api/rkm/#pyrkm.rkm.RKM.Deterministic_v_to_h","title":"<code>Deterministic_v_to_h(v, beta)</code>","text":"<p>Deterministically convert visible units to hidden units.</p> <p>Parameters:</p> <ul> <li> <code>v</code>               (<code>Tensor</code>)           \u2013            <p>Visible units.</p> </li> <li> <code>beta</code>               (<code>float</code>)           \u2013            <p>Inverse temperature parameter.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple of torch.Tensor</code>           \u2013            <p>Deterministic hidden units.</p> </li> </ul> Source code in <code>src/pyrkm/rkm.py</code> <pre><code>def Deterministic_v_to_h(self, v, beta):\n    \"\"\"Deterministically convert visible units to hidden units.\n\n    Parameters\n    ----------\n    v : torch.Tensor\n        Visible units.\n    beta : float\n        Inverse temperature parameter.\n\n    Returns\n    -------\n    tuple of torch.Tensor\n        Deterministic hidden units.\n    \"\"\"\n    h = (self.delta_eh(v) &gt; 0).to(v.dtype)\n    return h, h\n</code></pre>"},{"location":"api/rkm/#pyrkm.rkm.RKM.SGD_update","title":"<code>SGD_update(dEdW_data, dEdW_model, dEdv_bias_data, dEdv_bias_model, dEdh_bias_data, dEdh_bias_model)</code>","text":"<p>Update the model parameters using SGD.</p> <p>Parameters:</p> <ul> <li> <code>dEdW_data</code>               (<code>Tensor</code>)           \u2013            <p>Gradient of the weights from data.</p> </li> <li> <code>dEdW_model</code>               (<code>Tensor</code>)           \u2013            <p>Gradient of the weights from the model.</p> </li> <li> <code>dEdv_bias_data</code>               (<code>Tensor</code>)           \u2013            <p>Gradient of the visible biases from data.</p> </li> <li> <code>dEdv_bias_model</code>               (<code>Tensor</code>)           \u2013            <p>Gradient of the visible biases from the model.</p> </li> <li> <code>dEdh_bias_data</code>               (<code>Tensor</code>)           \u2013            <p>Gradient of the hidden biases from data.</p> </li> <li> <code>dEdh_bias_model</code>               (<code>Tensor</code>)           \u2013            <p>Gradient of the hidden biases from the model.</p> </li> </ul> Source code in <code>src/pyrkm/rkm.py</code> <pre><code>def SGD_update(self, dEdW_data, dEdW_model, dEdv_bias_data,\n               dEdv_bias_model, dEdh_bias_data, dEdh_bias_model):\n    \"\"\"Update the model parameters using SGD.\n\n    Parameters\n    ----------\n    dEdW_data : torch.Tensor\n        Gradient of the weights from data.\n    dEdW_model : torch.Tensor\n        Gradient of the weights from the model.\n    dEdv_bias_data : torch.Tensor\n        Gradient of the visible biases from data.\n    dEdv_bias_model : torch.Tensor\n        Gradient of the visible biases from the model.\n    dEdh_bias_data : torch.Tensor\n        Gradient of the hidden biases from data.\n    dEdh_bias_model : torch.Tensor\n        Gradient of the hidden biases from the model.\n    \"\"\"\n    # Gradients\n    dW = -dEdW_data + dEdW_model\n    dv = -dEdv_bias_data + dEdv_bias_model\n    dh = -dEdh_bias_data + dEdh_bias_model\n    if self.centering:\n        dv = dv - torch.matmul(self.oh, dW)\n        dh = dh - torch.matmul(self.ov, dW.t())\n    # Add regularization term\n    if self.regularization == 'l2':\n        dW -= self.l2 * 2 * self.W\n        dv -= self.l2 * 2 * self.v_bias\n        dh -= self.l2 * 2 * self.h_bias\n    elif self.regularization == 'l1':\n        dW -= self.l1 * torch.sign(self.W)\n        dv -= self.l1 * torch.sign(self.v_bias)\n        dh -= self.l1 * torch.sign(self.h_bias)\n    # Update parameters in-place\n    # # and clip\n    # gnorm = torch.norm(dW) + torch.norm(dv) + torch.norm(dh)\n    # myclip = (self.lr*10.) / gnorm if gnorm &gt; 10 else self.lr\n    self.W.add_(self.lr * dW)\n    self.v_bias.add_(self.lr * dv)\n    self.h_bias.add_(self.lr * dh)\n</code></pre>"},{"location":"api/rkm/#pyrkm.rkm.RKM.after_step_keepup","title":"<code>after_step_keepup()</code>","text":"<p>Perform operations after each training step.</p> Source code in <code>src/pyrkm/rkm.py</code> <pre><code>def after_step_keepup(self):\n    \"\"\"Perform operations after each training step.\"\"\"\n    # self.W_t = self.W.t() # already done in clip_weights\n    self.clip_weights()\n    self.clip_bias()\n</code></pre>"},{"location":"api/rkm/#pyrkm.rkm.RKM.av_power_backward","title":"<code>av_power_backward(h)</code>","text":"<p>Computes the average power dissipated by the RKM in the backward pass.</p> <p>Parameters:</p> <ul> <li> <code>h</code>               (<code>Tensor</code>)           \u2013            <p>Hidden units, shape (N, n_h).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Average power dissipated by the RKM.</p> </li> </ul> Source code in <code>src/pyrkm/rkm.py</code> <pre><code>def av_power_backward(self, h):\n    \"\"\"Computes the average power dissipated by the RKM in the backward pass.\n\n    Parameters\n    ----------\n    h : torch.Tensor\n        Hidden units, shape (N, n_h).\n\n    Returns\n    -------\n    torch.Tensor\n        Average power dissipated by the RKM.\n    \"\"\"\n    return self.power_backward(h).mean()\n</code></pre>"},{"location":"api/rkm/#pyrkm.rkm.RKM.av_power_forward","title":"<code>av_power_forward(v)</code>","text":"<p>Computes the average power dissipated by the RKM in the forward pass.</p> <p>Parameters:</p> <ul> <li> <code>v</code>               (<code>Tensor</code>)           \u2013            <p>Visible units, shape (N, n_v).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Average power dissipated by the RKM.</p> </li> </ul> Source code in <code>src/pyrkm/rkm.py</code> <pre><code>def av_power_forward(self, v):\n    \"\"\"Computes the average power dissipated by the RKM in the forward pass.\n\n    Parameters\n    ----------\n    v : torch.Tensor\n        Visible units, shape (N, n_v).\n\n    Returns\n    -------\n    torch.Tensor\n        Average power dissipated by the RKM.\n    \"\"\"\n    return self.power_forward(v).mean()\n</code></pre>"},{"location":"api/rkm/#pyrkm.rkm.RKM.clip_bias","title":"<code>clip_bias()</code>","text":"<p>Clip the biases to be within the specified range.</p> Source code in <code>src/pyrkm/rkm.py</code> <pre><code>def clip_bias(self):\n    \"\"\"Clip the biases to be within the specified range.\"\"\"\n    self.v_bias = torch.clip(self.v_bias, self.min_W, self.max_W)\n    self.h_bias = torch.clip(self.h_bias, self.min_W, self.max_W)\n</code></pre>"},{"location":"api/rkm/#pyrkm.rkm.RKM.clip_weights","title":"<code>clip_weights()</code>","text":"<p>Clip the weights to be within the specified range.</p> Source code in <code>src/pyrkm/rkm.py</code> <pre><code>def clip_weights(self):\n    \"\"\"Clip the weights to be within the specified range.\"\"\"\n    self.W = torch.clip(self.W, self.min_W, self.max_W)\n    self.W_t = self.W.t()\n</code></pre>"},{"location":"api/rkm/#pyrkm.rkm.RKM.delta_eh","title":"<code>delta_eh(v)</code>","text":"<p>Compute the change in energy for hidden units given visible units.</p> <p>Parameters:</p> <ul> <li> <code>v</code>               (<code>Tensor</code>)           \u2013            <p>Visible units.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Change in energy for hidden units.</p> </li> </ul> Source code in <code>src/pyrkm/rkm.py</code> <pre><code>def delta_eh(self, v):\n    \"\"\"Compute the change in energy for hidden units given visible units.\n\n    Parameters\n    ----------\n    v : torch.Tensor\n        Visible units.\n\n    Returns\n    -------\n    torch.Tensor\n        Change in energy for hidden units.\n    \"\"\"\n    if self.energy_type == 'hopfield':\n        return self._delta_eh_hopfield(v)\n    else:\n        # exit error\n        print('Error: delta_eh not implemented for this energy type')\n        sys.exit()\n</code></pre>"},{"location":"api/rkm/#pyrkm.rkm.RKM.delta_ev","title":"<code>delta_ev(h)</code>","text":"<p>Compute the change in energy for visible units given hidden units.</p> <p>Parameters:</p> <ul> <li> <code>h</code>               (<code>Tensor</code>)           \u2013            <p>Hidden units.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Change in energy for visible units.</p> </li> </ul> Source code in <code>src/pyrkm/rkm.py</code> <pre><code>def delta_ev(self, h):\n    \"\"\"Compute the change in energy for visible units given hidden units.\n\n    Parameters\n    ----------\n    h : torch.Tensor\n        Hidden units.\n\n    Returns\n    -------\n    torch.Tensor\n        Change in energy for visible units.\n    \"\"\"\n    if self.energy_type == 'hopfield':\n        return self._delta_ev_hopfield(h)\n    else:\n        # exit error\n        print('Error: delta_ev not implemented for this energy type')\n        sys.exit()\n</code></pre>"},{"location":"api/rkm/#pyrkm.rkm.RKM.derivatives","title":"<code>derivatives(v, h)</code>","text":"<p>Compute the derivatives for the specified energy type.</p> <p>Parameters:</p> <ul> <li> <code>v</code>               (<code>Tensor</code>)           \u2013            <p>Visible units.</p> </li> <li> <code>h</code>               (<code>Tensor</code>)           \u2013            <p>Hidden units.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple of torch.Tensor</code>           \u2013            <p>Gradients of the weights, visible biases, and hidden biases.</p> </li> </ul> Source code in <code>src/pyrkm/rkm.py</code> <pre><code>def derivatives(self, v, h):\n    \"\"\"Compute the derivatives for the specified energy type.\n\n    Parameters\n    ----------\n    v : torch.Tensor\n        Visible units.\n    h : torch.Tensor\n        Hidden units.\n\n    Returns\n    -------\n    tuple of torch.Tensor\n        Gradients of the weights, visible biases, and hidden biases.\n    \"\"\"\n    if self.energy_type == 'hopfield' or self.energy_type == 'RKM':\n        return self.derivatives_hopfield(v, h)\n    else:\n        # exit error\n        print('Error: derivatives not implemented for this energy type')\n        sys.exit()\n</code></pre>"},{"location":"api/rkm/#pyrkm.rkm.RKM.derivatives_hopfield","title":"<code>derivatives_hopfield(v, h)</code>","text":"<p>Compute the derivatives for the Hopfield energy.</p> <p>Parameters:</p> <ul> <li> <code>v</code>               (<code>Tensor</code>)           \u2013            <p>Visible units.</p> </li> <li> <code>h</code>               (<code>Tensor</code>)           \u2013            <p>Hidden units.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple of torch.Tensor</code>           \u2013            <p>Gradients of the weights, visible biases, and hidden biases.</p> </li> </ul> Source code in <code>src/pyrkm/rkm.py</code> <pre><code>def derivatives_hopfield(self, v, h):\n    \"\"\"Compute the derivatives for the Hopfield energy.\n\n    Parameters\n    ----------\n    v : torch.Tensor\n        Visible units.\n    h : torch.Tensor\n        Hidden units.\n\n    Returns\n    -------\n    tuple of torch.Tensor\n        Gradients of the weights, visible biases, and hidden biases.\n    \"\"\"\n    # h has shape (N, n_h) and v has shape (N, n_v), we want result to have shape (N, n_h, n_v)\n    if self.centering:\n        dEdW = -torch.einsum('ij,ik-&gt;ijk', h - self.oh, v - self.ov)\n    else:\n        dEdW = -torch.einsum('ij,ik-&gt;ijk', h, v)\n    dEdv_bias = -v\n    dEdh_bias = -h\n    return dEdW, dEdv_bias, dEdh_bias\n</code></pre>"},{"location":"api/rkm/#pyrkm.rkm.RKM.forward","title":"<code>forward(v, k, beta=None)</code>","text":"<p>Perform a forward pass through the network.</p> <p>Parameters:</p> <ul> <li> <code>v</code>               (<code>Tensor</code>)           \u2013            <p>Visible units.</p> </li> <li> <code>k</code>               (<code>int</code>)           \u2013            <p>Number of Gibbs sampling steps.</p> </li> <li> <code>beta</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Inverse temperature parameter, by default None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Reconstructed visible units.</p> </li> </ul> Source code in <code>src/pyrkm/rkm.py</code> <pre><code>def forward(self, v, k, beta=None):\n    \"\"\"Perform a forward pass through the network.\n\n    Parameters\n    ----------\n    v : torch.Tensor\n        Visible units.\n    k : int\n        Number of Gibbs sampling steps.\n    beta : float, optional\n        Inverse temperature parameter, by default None.\n\n    Returns\n    -------\n    torch.Tensor\n        Reconstructed visible units.\n    \"\"\"\n    if beta is None:\n        beta = self.model_beta\n    pre_h1, h1 = self.v_to_h(v, beta)\n    h_ = h1\n    for _ in range(k):\n        pre_v_, v_ = self.h_to_v(h_, beta)\n        pre_h_, h_ = self.v_to_h(v_, beta)\n    return v_\n</code></pre>"},{"location":"api/rkm/#pyrkm.rkm.RKM.generate","title":"<code>generate(n_samples, k, h_binarized=True, from_visible=True, beta=None)</code>","text":"<p>Generate samples from the model.</p> <p>Parameters:</p> <ul> <li> <code>n_samples</code>               (<code>int</code>)           \u2013            <p>Number of samples to generate.</p> </li> <li> <code>k</code>               (<code>int</code>)           \u2013            <p>Number of Gibbs sampling steps.</p> </li> <li> <code>h_binarized</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to binarize hidden units, by default True.</p> </li> <li> <code>from_visible</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to generate from visible units, by default True.</p> </li> <li> <code>beta</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Inverse temperature parameter, by default None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>Generated samples.</p> </li> </ul> Source code in <code>src/pyrkm/rkm.py</code> <pre><code>def generate(self,\n             n_samples,\n             k,\n             h_binarized=True,\n             from_visible=True,\n             beta=None):\n    \"\"\"Generate samples from the model.\n\n    Parameters\n    ----------\n    n_samples : int\n        Number of samples to generate.\n    k : int\n        Number of Gibbs sampling steps.\n    h_binarized : bool, optional\n        Whether to binarize hidden units, by default True.\n    from_visible : bool, optional\n        Whether to generate from visible units, by default True.\n    beta : float, optional\n        Inverse temperature parameter, by default None.\n\n    Returns\n    -------\n    numpy.ndarray\n        Generated samples.\n    \"\"\"\n    if beta is None:\n        beta = self.model_beta\n    if from_visible:\n        v = torch.randint(high=2,\n                          size=(n_samples, self.n_visible),\n                          device=self.device,\n                          dtype=self.mytype)\n    else:\n        if h_binarized:\n            h = torch.randint(high=2,\n                              size=(n_samples, self.n_hidden),\n                              device=self.device,\n                              dtype=self.mytype)\n        else:\n            h = torch.rand(n_samples,\n                           self.n_hidden,\n                           device=self.device,\n                           dtype=self.mytype)\n        _, v = self.h_to_v(h)\n    v_model = self.forward(v, k, beta)\n    return v_model.detach().cpu().numpy()\n</code></pre>"},{"location":"api/rkm/#pyrkm.rkm.RKM.h_to_v","title":"<code>h_to_v(h, beta=None)</code>","text":"<p>Convert hidden units to visible units.</p> <p>Parameters:</p> <ul> <li> <code>h</code>               (<code>Tensor</code>)           \u2013            <p>Hidden units.</p> </li> <li> <code>beta</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Inverse temperature parameter, by default None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple of torch.Tensor</code>           \u2013            <p>Probabilities and sampled visible units.</p> </li> </ul> Source code in <code>src/pyrkm/rkm.py</code> <pre><code>def h_to_v(self, h, beta=None):\n    \"\"\"Convert hidden units to visible units.\n\n    Parameters\n    ----------\n    h : torch.Tensor\n        Hidden units.\n    beta : float, optional\n        Inverse temperature parameter, by default None.\n\n    Returns\n    -------\n    tuple of torch.Tensor\n        Probabilities and sampled visible units.\n    \"\"\"\n    if beta is None:\n        beta = self.model_beta\n    if self.energy_type == 'RKM':\n        effective_v_bias = self.v_bias + 0.5 * self.offset * (\n            (torch.abs(self.v_bias) - self.v_bias) / self.g_v +\n            (torch.abs(self.W) - self.W).sum(dim=0))\n        num = torch.mm(h, self.W) + effective_v_bias\n        den = torch.abs(\n            self.W).sum(dim=0) + torch.abs(self.v_bias) / self.g_v\n        v_analog = num / den\n\n        if self.sampling == 'bernoulli':\n            if self.layer_scaled:\n                p_v = torch.sigmoid(beta * self.n_hidden * v_analog)\n                v = torch.bernoulli(p_v)\n            else:\n                p_v = torch.sigmoid(beta * v_analog)\n                v = torch.bernoulli(p_v)\n        elif self.sampling == 'multi-threshold':\n            if self.distribution == 'gaussian':\n                t = torch.randn_like(v_analog,\n                                     dtype=self.mytype,\n                                     device=self.device) * 1 / beta\n            else:\n                t = (torch.rand_like(\n                    v_analog, dtype=self.mytype, device=self.device) * 2 -\n                     1) * 1 / beta\n            p_v = v_analog\n            if self.layer_scaled:\n                v = (p_v &gt; t / self.n_hidden).to(h.dtype)\n            else:\n                v = (p_v &gt; t).to(h.dtype)\n        elif self.sampling == 'single-threshold':\n            if self.distribution == 'gaussian':\n                t = torch.randn(\n                    1, dtype=self.mytype,\n                    device=self.device) * 1 / beta * torch.ones_like(\n                        v_analog, dtype=self.mytype, device=self.device)\n            else:\n                t = (torch.rand(1, dtype=self.mytype, device=self.device) *\n                     2 - 1) * 1 / beta * torch.ones_like(\n                         v_analog, dtype=self.mytype, device=self.device)\n            p_v = v_analog\n            if self.layer_scaled:\n                v = (p_v &gt; t / self.n_hidden).to(h.dtype)\n            else:\n                v = (p_v &gt; t).to(h.dtype)\n        return p_v, v\n    else:\n        return super().h_to_v(h, beta)\n</code></pre>"},{"location":"api/rkm/#pyrkm.rkm.RKM.plot_bias","title":"<code>plot_bias(t)</code>","text":"<p>Plot the biases of the model.</p> <p>Parameters:</p> <ul> <li> <code>t</code>               (<code>int</code>)           \u2013            <p>Current epoch.</p> </li> </ul> Source code in <code>src/pyrkm/rkm.py</code> <pre><code>def plot_bias(self, t):\n    \"\"\"Plot the biases of the model.\n\n    Parameters\n    ----------\n    t : int\n        Current epoch.\n    \"\"\"\n    h_bias = self.h_bias.detach().cpu().numpy()\n    v_bias = self.v_bias.detach().cpu().numpy()\n    # Set up the figure with two subplots\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n    # Plot histogram for hidden biases\n    ax1.hist(h_bias, bins=20, color='blue', edgecolor='black')\n    ax1.set_xlabel('Values')\n    ax1.set_ylabel('Frequency')\n    ax1.set_title('Hidden Biases epoch {}'.format(t))\n    # Plot histogram for visible biases\n    ax2.hist(v_bias, bins=20, color='red', edgecolor='black')\n    ax2.set_xlabel('Values')\n    ax2.set_ylabel('Frequency')\n    ax2.set_title('Visible Biases epoch {}'.format(t))\n    # Adjust layout for better readability\n    plt.tight_layout()\n</code></pre>"},{"location":"api/rkm/#pyrkm.rkm.RKM.plot_visible_bias","title":"<code>plot_visible_bias(t)</code>","text":"<p>Plot the visible biases of the model.</p> <p>Parameters:</p> <ul> <li> <code>t</code>               (<code>int</code>)           \u2013            <p>Current epoch.</p> </li> </ul> Source code in <code>src/pyrkm/rkm.py</code> <pre><code>def plot_visible_bias(self, t):\n    \"\"\"Plot the visible biases of the model.\n\n    Parameters\n    ----------\n    t : int\n        Current epoch.\n    \"\"\"\n    # Reshape the vector into a 2D array\n    data_2d = self.v_bias.detach().cpu().numpy().reshape(28, 28)\n    # Create a figure and axis for the plot\n    fig, ax = plt.subplots(figsize=(5, 5))\n    # Plot the 2D array\n    im = ax.imshow(data_2d, cmap='magma')\n    # Add a colorbar\n    cbar = ax.figure.colorbar(im, ax=ax)\n    cbar.ax.set_ylabel('Values', rotation=-90, va='bottom')\n    # Add title and labels\n    ax.set_title('Visible Biases epoch {}'.format(t))\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Rows')\n</code></pre>"},{"location":"api/rkm/#pyrkm.rkm.RKM.plot_weights","title":"<code>plot_weights(t)</code>","text":"<p>Plot the weights of the model.</p> <p>Parameters:</p> <ul> <li> <code>t</code>               (<code>int</code>)           \u2013            <p>Current epoch.</p> </li> </ul> Source code in <code>src/pyrkm/rkm.py</code> <pre><code>def plot_weights(self, t):\n    \"\"\"Plot the weights of the model.\n\n    Parameters\n    ----------\n    t : int\n        Current epoch.\n    \"\"\"\n    Ndata = self.W.shape[0]\n    # Reshape the matrix into a 3D array\n    data_3d = self.W.detach().cpu().numpy().reshape(Ndata, 28, 28)\n    # Determine the number of rows and columns for the subplot grid\n    num_rows = int(np.ceil(np.sqrt(Ndata)))\n    num_cols = int(np.ceil(Ndata / num_rows))\n    # Create a figure and axis for the plot\n    fig, ax = plt.subplots(nrows=num_rows,\n                           ncols=num_cols,\n                           figsize=(10, 10))\n    # Iterate over the submatrices and plot them\n    for i in range(Ndata):\n        row = i // num_cols\n        col = i % num_cols\n        ax[row, col].imshow(data_3d[i], cmap='magma')\n        ax[row, col].axis('off')\n    # Remove empty subplots if the number of submatrices doesn't fill the entire grid\n    if num_rows * num_cols &gt; Ndata:\n        for i in range(Ndata, num_rows * num_cols):\n            row = i // num_cols\n            col = i % num_cols\n            fig.delaxes(ax[row, col])\n    # Adjust the spacing between subplots\n    plt.suptitle('Weights epoch {}'.format(t))\n    plt.subplots_adjust(wspace=0.05, hspace=0.05, top=0.9)\n    # Get the minimum and maximum values from the data\n    vmin = np.min(self.W.detach().cpu().numpy())\n    vmax = np.max(self.W.detach().cpu().numpy())\n    # Create a dummy image for the colorbar\n    dummy_img = np.zeros((1, 1))  # Dummy image with all zeros\n    # Add a colorbar using the dummy image as the mappable\n    cax = fig.add_axes([0.93, 0.15, 0.02, 0.7])  # Position of the colorbar\n    plt.colorbar(plt.imshow(dummy_img, cmap='magma', vmin=vmin, vmax=vmax),\n                 cax=cax)\n    # Adjust the height of the colorbar axes to match the height of the figure\n    cax.set_aspect('auto')\n</code></pre>"},{"location":"api/rkm/#pyrkm.rkm.RKM.power_backward","title":"<code>power_backward(h)</code>","text":"<p>Computes the power dissipated by the RKM in the backward pass.</p> <p>Parameters:</p> <ul> <li> <code>h</code>               (<code>Tensor</code>)           \u2013            <p>Hidden units, shape (N, n_h).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Power dissipated by the RKM, shape (N,).</p> </li> </ul> Source code in <code>src/pyrkm/rkm.py</code> <pre><code>def power_backward(self, h):\n    \"\"\"Computes the power dissipated by the RKM in the backward pass.\n\n    Parameters\n    ----------\n    h : torch.Tensor\n        Hidden units, shape (N, n_h).\n\n    Returns\n    -------\n    torch.Tensor\n        Power dissipated by the RKM, shape (N,).\n    \"\"\"\n    effective_v_bias = self.v_bias + 0.5 * self.offset * (\n        (torch.abs(self.v_bias) - self.v_bias) / self.g_v +\n        (torch.abs(self.W) - self.W).sum(dim=0))\n    num = torch.mm(h, self.W) + effective_v_bias\n    den = torch.abs(self.W).sum(dim=0) + torch.abs(self.v_bias) / self.g_v\n    v_analog = num / den\n\n    W_t = self.W_t\n    abs_W_t = torch.abs(self.W_t)\n    v_bias = self.v_bias\n    abs_v_bias = torch.abs(self.v_bias)\n\n    power_backward = (\n        -torch.einsum('ni,ij,nj-&gt;n', v_analog, W_t, h) + 0.5 *\n        (torch.einsum('ni,ij-&gt;n', v_analog**2, abs_W_t) +\n         torch.einsum('ij,nj-&gt;n', abs_W_t, h**2)) -\n        torch.einsum('i,ni-&gt;n', effective_v_bias, v_analog) +\n        (0.5 / self.g_v) *\n        torch.einsum('i,ni-&gt;n', abs_v_bias, v_analog**2 + self.g_v**2))\n\n    if self.offset != 0:\n        power_backward = power_backward + (\n            +torch.einsum('ij,nj-&gt;n', abs_W_t - W_t,\n                          (-2 * self.offset * h + self.offset**2 / 4)) +\n            (self.offset**2 / (4 * self.g_v) - self.offset / 2) *\n            torch.sum(abs_v_bias - v_bias))\n\n    return power_backward\n</code></pre>"},{"location":"api/rkm/#pyrkm.rkm.RKM.power_forward","title":"<code>power_forward(v)</code>","text":"<p>Computes the power dissipated by the RKM in the forward pass.</p> <p>Parameters:</p> <ul> <li> <code>v</code>               (<code>Tensor</code>)           \u2013            <p>Visible units, shape (N, n_v).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Power dissipated by the RKM, shape (N,).</p> </li> </ul> Source code in <code>src/pyrkm/rkm.py</code> <pre><code>def power_forward(self, v):\n    \"\"\"Computes the power dissipated by the RKM in the forward pass.\n\n    Parameters\n    ----------\n    v : torch.Tensor\n        Visible units, shape (N, n_v).\n\n    Returns\n    -------\n    torch.Tensor\n        Power dissipated by the RKM, shape (N,).\n    \"\"\"\n    effective_h_bias = self.h_bias + 0.5 * self.offset * (\n        (torch.abs(self.h_bias) - self.h_bias) / self.g_h +\n        (torch.abs(self.W) - self.W).sum(dim=1))\n    num = torch.mm(v, self.W_t) + effective_h_bias\n    den = torch.abs(self.W).sum(dim=1) + torch.abs(self.h_bias) / self.g_h\n    h_analog = num / den\n\n    W_t = self.W_t\n    abs_W_t = torch.abs(self.W_t)\n    h_bias = self.h_bias\n    abs_h_bias = torch.abs(self.h_bias)\n\n    power_forward = (\n        -torch.einsum('ni,ij,nj-&gt;n', v, W_t, h_analog) + 0.5 *\n        (torch.einsum('ni,ij-&gt;n', v**2, abs_W_t) +\n         torch.einsum('ij,nj-&gt;n', abs_W_t, h_analog**2)) -\n        torch.einsum('j,nj-&gt;n', effective_h_bias, h_analog) +\n        (0.5 / self.g_h) *\n        torch.einsum('j,nj-&gt;n', abs_h_bias, h_analog**2 + self.g_h**2))\n\n    if self.offset != 0:\n        power_forward = power_forward + (+torch.einsum(\n            'ni,ij-&gt;n',\n            (-2 * self.offset * v + self.offset**2 / 4), abs_W_t - W_t) +\n                                         (self.offset**2 /\n                                          (4 * self.g_h) - self.offset / 2)\n                                         * torch.sum(abs_h_bias - h_bias))\n\n    return power_forward\n</code></pre>"},{"location":"api/rkm/#pyrkm.rkm.RKM.pretrain","title":"<code>pretrain(pretrained_model, model_state_path='model_states/')</code>","text":"<p>Pretrain the model using a pretrained model.</p> <p>Parameters:</p> <ul> <li> <code>pretrained_model</code>               (<code>str</code>)           \u2013            <p>Name of the pretrained model.</p> </li> <li> <code>model_state_path</code>               (<code>str</code>, default:                   <code>'model_states/'</code> )           \u2013            <p>Path to the model states, by default 'model_states/'.</p> </li> </ul> Source code in <code>src/pyrkm/rkm.py</code> <pre><code>def pretrain(self, pretrained_model, model_state_path='model_states/'):\n    \"\"\"Pretrain the model using a pretrained model.\n\n    Parameters\n    ----------\n    pretrained_model : str\n        Name of the pretrained model.\n    model_state_path : str, optional\n        Path to the model states, by default 'model_states/'.\n    \"\"\"\n    # Check if you have model load points\n    ensure_dir(model_state_path)\n    filename_list = glob.glob(model_state_path +\n                              '{}_t*.pkl'.format(pretrained_model))\n    if len(filename_list) &gt; 0:\n        all_loadpoints = sorted([\n            int(x.split('_t')[-1].split('.pkl')[0]) for x in filename_list\n        ])\n        last_epoch = all_loadpoints[-1]\n        print('** Using as pretraining model {} at epoch {}'.format(\n            pretrained_model, last_epoch),\n              flush=True)\n        with open(\n                model_state_path +\n                '{}_t{}.pkl'.format(pretrained_model, last_epoch),\n                'rb') as file:\n            temp_model = pickle.load(file)\n            # *** Import pretrained parameters\n            self.W = temp_model.W.to(self.mytype)\n            self.h_bias = temp_model.h_bias.to(self.mytype)\n            self.v_bias = temp_model.v_bias.to(self.mytype)\n    else:\n        print('** No load points for {}'.format(pretrained_model),\n              flush=True)\n</code></pre>"},{"location":"api/rkm/#pyrkm.rkm.RKM.reconstruct","title":"<code>reconstruct(data, k)</code>","text":"<p>Reconstruct the visible units from the data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>array - like</code>)           \u2013            <p>Input data.</p> </li> <li> <code>k</code>               (<code>int</code>)           \u2013            <p>Number of Gibbs sampling steps.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple of numpy.ndarray</code>           \u2013            <p>Original and reconstructed visible units.</p> </li> </ul> Source code in <code>src/pyrkm/rkm.py</code> <pre><code>def reconstruct(self, data, k):\n    \"\"\"Reconstruct the visible units from the data.\n\n    Parameters\n    ----------\n    data : array-like\n        Input data.\n    k : int\n        Number of Gibbs sampling steps.\n\n    Returns\n    -------\n    tuple of numpy.ndarray\n        Original and reconstructed visible units.\n    \"\"\"\n    data = torch.Tensor(data).to(self.device).to(self.mytype)\n    v_model = self.forward(data, k)\n    return data.detach().cpu().numpy(), v_model.detach().cpu().numpy()\n</code></pre>"},{"location":"api/rkm/#pyrkm.rkm.RKM.relaxation_times","title":"<code>relaxation_times()</code>","text":"<p>Computes the relaxation times of the RKM in the forward and backward pass.</p> <p>Returns:</p> <ul> <li> <code>tuple of torch.Tensor</code>           \u2013            <p>t_forward : relaxation times of the RKM in the forward pass, shape (n_v,). t_backward : relaxation times of the RKM in the backward pass, shape (n_h,).</p> </li> </ul> Source code in <code>src/pyrkm/rkm.py</code> <pre><code>def relaxation_times(self):\n    \"\"\"Computes the relaxation times of the RKM in the forward and backward pass.\n\n    Returns\n    -------\n    tuple of torch.Tensor\n        t_forward : relaxation times of the RKM in the forward pass, shape (n_v,).\n        t_backward : relaxation times of the RKM in the backward pass, shape (n_h,).\n    \"\"\"\n    t_forward = 2 / (torch.abs(self.W).sum(dim=1) +\n                     torch.abs(self.h_bias) / self.g_h)\n    t_backward = 2 / (torch.abs(self.W).sum(dim=0) +\n                      torch.abs(self.v_bias) / self.g_v)\n\n    return t_forward, t_backward\n</code></pre>"},{"location":"api/rkm/#pyrkm.rkm.RKM.train","title":"<code>train(train_data, test_data=[], print_error=False, print_test_error=False, model_state_path='model_states/', print_every=100)</code>","text":"<p>Train the model using the given data and parameters.</p> <p>Parameters:</p> <ul> <li> <code>train_data</code>               (<code>Tensor</code>)           \u2013            <p>Training data.</p> </li> <li> <code>test_data</code>               (<code>Tensor</code>, default:                   <code>[]</code> )           \u2013            <p>Test data, by default [].</p> </li> <li> <code>print_error</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to print training error, by default False.</p> </li> <li> <code>print_test_error</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to print test error, by default False.</p> </li> <li> <code>model_state_path</code>               (<code>str</code>, default:                   <code>'model_states/'</code> )           \u2013            <p>Path to save model states, by default 'model_states/'.</p> </li> <li> <code>print_every</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Frequency of printing progress, by default 100.</p> </li> </ul> Source code in <code>src/pyrkm/rkm.py</code> <pre><code>def train(self,\n          train_data,\n          test_data=[],\n          print_error=False,\n          print_test_error=False,\n          model_state_path='model_states/',\n          print_every=100):\n    \"\"\"Train the model using the given data and parameters.\n\n    Parameters\n    ----------\n    train_data : torch.Tensor\n        Training data.\n    test_data : torch.Tensor, optional\n        Test data, by default [].\n    print_error : bool, optional\n        Whether to print training error, by default False.\n    print_test_error : bool, optional\n        Whether to print test error, by default False.\n    model_state_path : str, optional\n        Path to save model states, by default 'model_states/'.\n    print_every : int, optional\n        Frequency of printing progress, by default 100.\n    \"\"\"\n    while self.epoch &lt; self.max_epochs:\n        self.W_t = self.W.t()\n\n        for _, v_data in enumerate(train_data):\n\n            start_time = time.time()\n            # restart the power\n            self.power_f = 0\n            self.power_b = 0\n\n            # For the positive phase, we use the data and propagate to the hidden nodes\n            h_data = self.v_to_h(v_data)[1]\n            p_f = self.power_forward(v_data)\n            self.power_f += p_f.mean()\n            self.energy += p_f.sum()\n\n            # For the negative phase, it depends on the training algorithm\n            if self.train_algo == 'PCD':\n                # Update the chain after every batch\n                # self.persistent_chains = self.forward(\n                #     self.persistent_chains, self.k)\n                v_model = self.persistent_chains\n                # print(\n                #     'Warning: No physical measurements are implemented for PCD training algorithm.'\n                # )\n                for _ in range(self.k):\n                    h_model = self.v_to_h(v_model)[1]\n                    p_f = self.power_forward(v_model)\n                    self.power_f += p_f.mean()\n\n                    v_model = self.h_to_v(h_model)[1]\n                    p_b = self.power_backward(h_model)\n                    self.power_b += p_b.mean()\n\n                    self.energy += p_f.sum() + p_b.sum()\n\n                self.persistent_chains = v_model\n            elif self.train_algo == 'RDM':\n                v_model = torch.randint(high=2,\n                                        size=(self.batch_size,\n                                              self.n_visible),\n                                        device=self.device,\n                                        dtype=self.mytype)\n                v_model = self.forward(v_model, self.k)\n                print(\n                    'Warning: No physical measurements are implemented for RDM training algorithm.'\n                    + 'Use hRDM or vRDM instead.')\n            elif self.train_algo == 'CD':\n                v_model = v_data\n                for _ in range(self.k):\n                    h_model = self.v_to_h(v_model, self.model_beta)[1]\n                    p_f = self.power_forward(v_model)\n                    self.power_f += p_f.mean()\n\n                    v_model = self.h_to_v(h_model, self.model_beta)[1]\n                    p_b = self.power_backward(h_model)\n                    self.power_b += p_b.mean()\n\n                    self.energy += p_f.sum() + p_b.sum()\n            elif self.train_algo == 'vRDM':\n                # visible RDM\n                v_model = torch.randint(high=2,\n                                        size=(self.batch_size,\n                                              self.n_visible),\n                                        device=self.device,\n                                        dtype=self.mytype)\n                for _ in range(self.k):\n                    h_model = self.v_to_h(v_model, self.model_beta)[1]\n                    p_f = self.power_forward(v_model)\n                    self.power_f += p_f.mean()\n\n                    v_model = self.h_to_v(h_model, self.model_beta)[1]\n                    p_b = self.power_backward(h_model)\n                    self.power_b += p_b.mean()\n\n                    self.energy += p_f.sum() + p_b.sum()\n            elif self.train_algo == 'hRDM':\n                # hidden RDM\n                h_model = torch.randint(high=2,\n                                        size=(self.batch_size,\n                                              self.n_hidden),\n                                        device=self.device,\n                                        dtype=self.mytype)\n                v_model = self.h_to_v(h_model, self.model_beta)[1]\n                p_b = self.power_backward(h_model)\n                self.power_b += p_b.mean()\n\n                self.energy += p_b.sum()\n\n                for _ in range(self.k - 1):\n                    h_model = self.v_to_h(v_model, self.model_beta)[1]\n                    p_f = self.power_forward(v_model)\n                    self.power_f += p_f.mean()\n\n                    v_model = self.h_to_v(h_model, self.model_beta)[1]\n                    p_b = self.power_backward(h_model)\n                    self.power_b += p_b.mean()\n\n                    self.energy += p_f.sum() + p_b.sum()\n\n            # Apply centering\n            if self.centering:\n                self.batch_ov = v_data.mean(0)\n                self.batch_oh = h_data.mean(0)\n                # update with sliding\n                self.ov = (1 -\n                           self.slv) * self.ov + self.slv * self.batch_ov\n                self.oh = (1 -\n                           self.slh) * self.oh + self.slh * self.batch_oh\n\n            # Compute gradients\n            dEdW_data, dEdv_bias_data, dEdh_bias_data = self.derivatives(\n                v_data, h_data)\n            dEdW_model, dEdv_bias_model, dEdh_bias_model = self.derivatives(\n                v_model, h_model)\n\n            # Average over batch\n            dEdW_data = torch.mean(dEdW_data, dim=0)\n            dEdv_bias_data = torch.mean(dEdv_bias_data, dim=0)\n            dEdh_bias_data = torch.mean(dEdh_bias_data, dim=0)\n            dEdW_model = torch.mean(dEdW_model, dim=0)\n            dEdv_bias_model = torch.mean(dEdv_bias_model, dim=0)\n            dEdh_bias_model = torch.mean(dEdh_bias_model, dim=0)\n\n            # Update weights and biases\n            if self.optimizer == 'Adam':\n                self.Adam_update(self.epoch + 1, dEdW_data, dEdW_model,\n                                 dEdv_bias_data, dEdv_bias_model,\n                                 dEdh_bias_data, dEdh_bias_model)\n            elif self.optimizer == 'SGD':\n                self.SGD_update(dEdW_data, dEdW_model, dEdv_bias_data,\n                                dEdv_bias_model, dEdh_bias_data,\n                                dEdh_bias_model)\n\n            self.after_step_keepup()\n\n            # compute new relaxation times\n            self.relax_t_f, self.relax_t_b = self.relaxation_times()\n\n            self.epoch += 1\n\n            # Store the model state\n            if self.epoch in self.t_to_save:\n                ensure_dir(model_state_path)\n                with open(\n                        model_state_path +\n                        '{}_t{}.pkl'.format(self.model_name, self.epoch),\n                        'wb') as file:\n                    pickle.dump(self, file)\n\n            if self.epoch % print_every == 0:\n                t = time.time() - start_time\n                if print_error:\n                    v_model = self.forward(v_data, 1)\n                    rec_error_train = ((v_model -\n                                        v_data)**2).mean(1).mean(0)\n                    if not print_test_error:\n                        print('Epoch: %d , train-err %.5g , time: %f' %\n                              (self.epoch, rec_error_train, t),\n                              flush=True)\n                    else:\n                        t_model = self.forward(test_data, 1)\n                        rec_error_test = ((t_model -\n                                           test_data)**2).mean(1).mean(0)\n                        print(\n                            'Epoch: %d , Test-err %.5g , train-err %.5g , time: %f'\n                            % (self.epoch, rec_error_test, rec_error_train,\n                               t),\n                            flush=True)\n                else:\n                    print('Epoch: %d , time: %f' % (self.epoch, t),\n                          flush=True)\n\n    print('*** Training finished', flush=True)\n</code></pre>"},{"location":"api/rkm/#pyrkm.rkm.RKM.v_to_h","title":"<code>v_to_h(v, beta=None)</code>","text":"<p>Convert visible units to hidden units.</p> <p>Parameters:</p> <ul> <li> <code>v</code>               (<code>Tensor</code>)           \u2013            <p>Visible units.</p> </li> <li> <code>beta</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Inverse temperature parameter, by default None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple of torch.Tensor</code>           \u2013            <p>Probabilities and sampled hidden units.</p> </li> </ul> Source code in <code>src/pyrkm/rkm.py</code> <pre><code>def v_to_h(self, v, beta=None):\n    \"\"\"Convert visible units to hidden units.\n\n    Parameters\n    ----------\n    v : torch.Tensor\n        Visible units.\n    beta : float, optional\n        Inverse temperature parameter, by default None.\n\n    Returns\n    -------\n    tuple of torch.Tensor\n        Probabilities and sampled hidden units.\n    \"\"\"\n    if beta is None:\n        beta = self.model_beta\n    if self.energy_type == 'RKM':\n        effective_h_bias = self.h_bias + 0.5 * self.offset * (\n            (torch.abs(self.h_bias) - self.h_bias) / self.g_h +\n            (torch.abs(self.W) - self.W).sum(dim=1))\n        num = torch.mm(v, self.W_t) + effective_h_bias\n        den = torch.abs(\n            self.W).sum(dim=1) + torch.abs(self.h_bias) / self.g_h\n        h_analog = num / den\n\n        if self.sampling == 'bernoulli':\n            if self.layer_scaled:\n                p_h = torch.sigmoid(beta * self.n_visible * h_analog)\n                h = torch.bernoulli(p_h)\n            else:\n                p_h = torch.sigmoid(beta * h_analog)\n                h = torch.bernoulli(p_h)\n        elif self.sampling == 'multi-threshold':\n            if self.distribution == 'gaussian':\n                t = torch.randn_like(h_analog,\n                                     dtype=self.mytype,\n                                     device=self.device) * 1 / beta\n            else:\n                t = (torch.rand_like(\n                    h_analog, dtype=self.mytype, device=self.device) * 2 -\n                     1) * 1 / beta\n            p_h = h_analog\n            if self.layer_scaled:\n                h = (p_h &gt; t / self.n_visible).to(v.dtype)\n            else:\n                h = (p_h &gt; t).to(v.dtype)\n        elif self.sampling == 'single-threshold':\n            if self.distribution == 'gaussian':\n                t = torch.randn(\n                    1, dtype=self.mytype,\n                    device=self.device) * 1 / beta * torch.ones_like(\n                        h_analog, dtype=self.mytype, device=self.device)\n            else:\n                t = (torch.rand(1, dtype=self.mytype, device=self.device) *\n                     2 - 1) * 1 / beta * torch.ones_like(\n                         h_analog, dtype=self.mytype, device=self.device)\n            p_h = h_analog\n            if self.layer_scaled:\n                h = (p_h &gt; t / self.n_visible).to(v.dtype)\n            else:\n                h = (p_h &gt; t).to(v.dtype)\n        return p_h, h\n    else:\n        return super().v_to_h(v, beta)\n</code></pre>"},{"location":"api/utils/","title":"utils","text":""},{"location":"api/utils/#pyrkm.utils.ComputeAATS","title":"<code>ComputeAATS(v, v_model)</code>","text":"<p>Compute the Average Absolute Truth Score (AATS) between original and model data.</p> <p>Parameters:</p> <ul> <li> <code>v</code>               (<code>Tensor</code>)           \u2013            <p>The original data.</p> </li> <li> <code>v_model</code>               (<code>Tensor</code>)           \u2013            <p>The model data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>The AATS for true samples.</p> </li> <li> <code>float</code>           \u2013            <p>The AATS for synthetic samples.</p> </li> </ul> Source code in <code>src/pyrkm/utils.py</code> <pre><code>def ComputeAATS(v, v_model):\n    \"\"\"Compute the Average Absolute Truth Score (AATS) between original and model data.\n\n    Parameters\n    ----------\n    v : torch.Tensor\n        The original data.\n    v_model : torch.Tensor\n        The model data.\n\n    Returns\n    -------\n    float\n        The AATS for true samples.\n    float\n        The AATS for synthetic samples.\n    \"\"\"\n    CONCAT = torch.cat((v, v_model), 1)\n    dAB = torch.cdist(CONCAT.t(), CONCAT.t())\n    torch.diagonal(dAB).fill_(float('inf'))\n    dAB = dAB.cpu().numpy()\n\n    # the next line is use to tranform the matrix into\n    #  d_TT d_TF   INTO d_TF- d_TT-  where the minus indicate a reverse order of the columns\n    #  d_FT d_FF        d_FT  d_FF\n    dAB[:int(dAB.shape[0] / 2), :] = dAB[:int(dAB.shape[0] / 2), ::-1]\n    closest = dAB.argmin(axis=1)\n    n = int(closest.shape[0] / 2)\n\n    ninv = 1 / n\n    AAtruth = (closest[:n] &gt;= n).sum() * ninv\n    AAsyn = (closest[n:] &gt;= n).sum() * ninv\n\n    return AAtruth, AAsyn\n</code></pre>"},{"location":"api/utils/#pyrkm.utils.Compute_FID","title":"<code>Compute_FID(synthetic_images, real_images)</code>","text":"<p>Compute the Frechet Inception Distance (FID) between synthetic and real images.</p> <p>Parameters:</p> <ul> <li> <code>synthetic_images</code>               (<code>Tensor</code>)           \u2013            <p>The synthetic images.</p> </li> <li> <code>real_images</code>               (<code>Tensor</code>)           \u2013            <p>The real images.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>The FID score.</p> </li> </ul> Source code in <code>src/pyrkm/utils.py</code> <pre><code>def Compute_FID(synthetic_images, real_images):\n    \"\"\"Compute the Frechet Inception Distance (FID) between synthetic and real images.\n\n    Parameters\n    ----------\n    synthetic_images : torch.Tensor\n        The synthetic images.\n    real_images : torch.Tensor\n        The real images.\n\n    Returns\n    -------\n    float\n        The FID score.\n    \"\"\"\n    device = synthetic_images.device\n    inception_model = inception_v3(pretrained=True,\n                                   transform_input=False).to(device)\n    inception_model.eval()\n\n    def preprocess_images(images):\n        images = images.reshape(-1, 28,\n                                28).unsqueeze(1).repeat(1, 3, 1,\n                                                        1).to(torch.float64)\n        images = torch.nn.functional.interpolate(images,\n                                                 size=299,\n                                                 mode='bilinear',\n                                                 align_corners=False)\n        return images\n\n    def get_activations(images):\n        images = preprocess_images(images)\n        with torch.no_grad():\n            return inception_model(images).detach().cpu().numpy()\n\n    synthetic_images = synthetic_images.to(device)\n    real_images = torch.Tensor(real_images).to(device)\n    synthetic_activations = get_activations(synthetic_images)\n    real_activations = get_activations(real_images)\n\n    mu_synthetic = np.mean(synthetic_activations, axis=0)\n    mu_real = np.mean(real_activations, axis=0)\n    sigma_synthetic = np.cov(synthetic_activations, rowvar=False)\n    sigma_real = np.cov(real_activations, rowvar=False)\n\n    epsilon = 1e-6\n    sigma_synthetic += np.eye(sigma_synthetic.shape[0]) * epsilon\n    sigma_real += np.eye(sigma_real.shape[0]) * epsilon\n\n    diff = mu_synthetic - mu_real\n    ssdiff = np.sum(diff**2.0)\n\n    covmean = sqrtm(sigma_synthetic.dot(sigma_real))\n    if np.iscomplexobj(covmean):\n        covmean = covmean.real\n\n    fid = ssdiff + np.trace(sigma_synthetic + sigma_real - 2.0 * covmean)\n    return fid\n</code></pre>"},{"location":"api/utils/#pyrkm.utils.Compute_S","title":"<code>Compute_S(v, v_gen)</code>","text":"<p>Compute the relative entropy between original and generated data.</p> <p>Parameters:</p> <ul> <li> <code>v</code>               (<code>Tensor</code>)           \u2013            <p>The original data.</p> </li> <li> <code>v_gen</code>               (<code>Tensor</code>)           \u2013            <p>The generated data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>The relative entropy.</p> </li> </ul> Source code in <code>src/pyrkm/utils.py</code> <pre><code>def Compute_S(v, v_gen):\n    \"\"\"Compute the relative entropy between\n    original and generated data.\n\n    Parameters\n    ----------\n    v : torch.Tensor\n        The original data.\n    v_gen : torch.Tensor\n        The generated data.\n\n    Returns\n    -------\n    float\n        The relative entropy.\n    \"\"\"\n    v = v.detach().cpu().numpy()\n    try:\n        v_gen = v_gen.detach().cpu().numpy()\n    except Exception:\n        v_gen = v_gen\n\n    # define a mixed set for crossentropy:\n    # this set will contain the first half of the original set and the second half of the generated set\n    v_cross = v.copy()\n    v_cross[:int(0.5 * v_cross.shape[0])] = v_gen[:int(0.5 * v_cross.shape[0])]\n\n    # Convert the array to bytes\n    bytes_io = io.BytesIO()\n    np.save(bytes_io, v)\n    bytes_src = bytes_io.getvalue()\n    np.save(bytes_io, v_cross)\n    bytes_cross = bytes_io.getvalue()\n\n    # Compress the bytes using gzip\n    compressed_src = gzip.compress(bytes_src)\n    compressed_cross = gzip.compress(bytes_cross)\n\n    # Calculate the entropy\n    byte_count_src = len(compressed_src)\n    byte_count_cross = len(compressed_cross)\n    value_counts_src = np.bincount(\n        np.frombuffer(compressed_src, dtype=np.uint8))\n    value_counts_cross = np.bincount(\n        np.frombuffer(compressed_cross, dtype=np.uint8))\n    probabilities_src = value_counts_src / byte_count_src\n    probabilities_cross = value_counts_cross / byte_count_cross\n    probabilities_src = probabilities_src[probabilities_src &gt; 0]\n    probabilities_cross = probabilities_cross[probabilities_cross &gt; 0]\n    entropy_src = -np.sum(probabilities_src * np.log2(probabilities_src))\n    entropy_cross = -np.sum(probabilities_cross * np.log2(probabilities_cross))\n\n    # the final measure is this relative entropy that is centered around 0.\n    return entropy_cross / entropy_src - 1\n</code></pre>"},{"location":"api/utils/#pyrkm.utils.Covariance_error","title":"<code>Covariance_error(centered_data_original, centered_data_model, Nv)</code>","text":"<p>Compute the covariance error between original and model data.</p> <p>Parameters:</p> <ul> <li> <code>centered_data_original</code>               (<code>Tensor</code>)           \u2013            <p>The centered original data.</p> </li> <li> <code>centered_data_model</code>               (<code>Tensor</code>)           \u2013            <p>The centered model data.</p> </li> <li> <code>Nv</code>               (<code>int</code>)           \u2013            <p>The number of visible units.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>The covariance error.</p> </li> </ul> Source code in <code>src/pyrkm/utils.py</code> <pre><code>def Covariance_error(centered_data_original, centered_data_model, Nv):\n    \"\"\"Compute the covariance error between original and model data.\n\n    Parameters\n    ----------\n    centered_data_original : torch.Tensor\n        The centered original data.\n    centered_data_model : torch.Tensor\n        The centered model data.\n    Nv : int\n        The number of visible units.\n\n    Returns\n    -------\n    torch.Tensor\n        The covariance error.\n    \"\"\"\n    covariance_matrix_original = torch.matmul(centered_data_original.T,\n                                              centered_data_original).mean(0)\n    covariance_matrix_model = torch.matmul(centered_data_model.T,\n                                           centered_data_model).mean(0)\n    return torch.pow(covariance_matrix_original - covariance_matrix_model,\n                     2).triu().sum() * 2 / (Nv * (Nv - 1))\n</code></pre>"},{"location":"api/utils/#pyrkm.utils.PowerSpectrum_MSE","title":"<code>PowerSpectrum_MSE(v, v_model)</code>","text":"<p>Compute the mean squared error of the power spectrum between original and model data.</p> <p>Parameters:</p> <ul> <li> <code>v</code>               (<code>Tensor</code>)           \u2013            <p>The original data.</p> </li> <li> <code>v_model</code>               (<code>Tensor</code>)           \u2013            <p>The model data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>The mean squared error of the power spectrum.</p> </li> </ul> Source code in <code>src/pyrkm/utils.py</code> <pre><code>def PowerSpectrum_MSE(v, v_model):\n    \"\"\"Compute the mean squared error of the power spectrum between original and model data.\n\n    Parameters\n    ----------\n    v : torch.Tensor\n        The original data.\n    v_model : torch.Tensor\n        The model data.\n\n    Returns\n    -------\n    torch.Tensor\n        The mean squared error of the power spectrum.\n    \"\"\"\n    # Apply 2D FFT to the signal\n    signal_fft_original = fft.fft2(v)\n    signal_fft_model = fft.fft2(v_model)\n    # Compute the power spectrum\n    power_spectrum_original = torch.mean(torch.abs(signal_fft_original)**2, 0)\n    power_spectrum_model = torch.mean(torch.abs(signal_fft_model)**2, 0)\n    # MSE of the power spectrum\n    return torch.mean((torch.log(power_spectrum_original) -\n                       torch.log(power_spectrum_model))**2)\n</code></pre>"},{"location":"api/utils/#pyrkm.utils.Third_moment_error","title":"<code>Third_moment_error(centered_data_original, centered_data_model, Nv)</code>","text":"<p>Compute the third moment error between original and model data.</p> <p>Parameters:</p> <ul> <li> <code>centered_data_original</code>               (<code>Tensor</code>)           \u2013            <p>The centered original data.</p> </li> <li> <code>centered_data_model</code>               (<code>Tensor</code>)           \u2013            <p>The centered model data.</p> </li> <li> <code>Nv</code>               (<code>int</code>)           \u2013            <p>The number of visible units.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>The third moment error.</p> </li> </ul> Source code in <code>src/pyrkm/utils.py</code> <pre><code>def Third_moment_error(centered_data_original, centered_data_model, Nv):\n    \"\"\"Compute the third moment error between original and model data.\n\n    Parameters\n    ----------\n    centered_data_original : torch.Tensor\n        The centered original data.\n    centered_data_model : torch.Tensor\n        The centered model data.\n    Nv : int\n        The number of visible units.\n\n    Returns\n    -------\n    torch.Tensor\n        The third moment error.\n    \"\"\"\n    C_ijk_original = torch.einsum(\n        'ni,nj,nk-&gt;ijk', centered_data_original, centered_data_original,\n        centered_data_original) / centered_data_model.shape[0]\n    C_ijk_model = torch.einsum(\n        'ni,nj,nk-&gt;ijk', centered_data_model, centered_data_model,\n        centered_data_model) / centered_data_model.shape[0]\n    C_ijk = torch.pow(C_ijk_model - C_ijk_original, 2)\n    sum_ijk = 0.0\n    upper_triangular = torch.triu(C_ijk, diagonal=1)\n    sum_ijk = upper_triangular.sum(dim=(0, 1, 2))\n    return sum_ijk * 6 / (Nv * (Nv - 1) * (Nv - 2))\n</code></pre>"},{"location":"api/utils/#pyrkm.utils.binarize_image","title":"<code>binarize_image(image, threshold=128)</code>","text":"<p>Binarize the image using a threshold.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>ndarray</code>)           \u2013            <p>The input image.</p> </li> <li> <code>threshold</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>The threshold value (default is 128).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The binarized image.</p> </li> </ul> Source code in <code>src/pyrkm/utils.py</code> <pre><code>def binarize_image(image, threshold=128):\n    \"\"\"Binarize the image using a threshold.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        The input image.\n    threshold : int, optional\n        The threshold value (default is 128).\n\n    Returns\n    -------\n    numpy.ndarray\n        The binarized image.\n    \"\"\"\n    binary_image = (image &gt; threshold).astype(int)\n    return binary_image\n</code></pre>"},{"location":"api/utils/#pyrkm.utils.ensure_dir","title":"<code>ensure_dir(dirname)</code>","text":"<p>Create a directory if it does not exist.</p> <p>Parameters:</p> <ul> <li> <code>dirname</code>               (<code>str</code>)           \u2013            <p>The name of the directory to create.</p> </li> </ul> Source code in <code>src/pyrkm/utils.py</code> <pre><code>def ensure_dir(dirname):\n    \"\"\"Create a directory if it does not exist.\n\n    Parameters\n    ----------\n    dirname : str\n        The name of the directory to create.\n    \"\"\"\n    if not os.path.exists(dirname):\n        os.makedirs(dirname)\n</code></pre>"},{"location":"api/utils/#pyrkm.utils.generate_S_matrix","title":"<code>generate_S_matrix(shape, target)</code>","text":"<p>Generate a random matrix with values between 0 and 1, adjusted to achieve the desired average.</p> <p>Parameters:</p> <ul> <li> <code>shape</code>               (<code>tuple</code>)           \u2013            <p>The shape of the matrix.</p> </li> <li> <code>target</code>               (<code>float</code>)           \u2013            <p>The target average value.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The generated matrix.</p> </li> </ul> Source code in <code>src/pyrkm/utils.py</code> <pre><code>def generate_S_matrix(shape, target):\n    \"\"\"Generate a random matrix with values between 0 and 1, adjusted to achieve the desired average.\n\n    Parameters\n    ----------\n    shape : tuple\n        The shape of the matrix.\n    target : float\n        The target average value.\n\n    Returns\n    -------\n    numpy.ndarray\n        The generated matrix.\n    \"\"\"\n    random_matrix = np.random.rand(*shape)\n    adjusted_matrix = random_matrix + (target - np.mean(random_matrix))\n    adjusted_matrix = np.clip(adjusted_matrix, 0, 1)\n    return adjusted_matrix\n</code></pre>"},{"location":"api/utils/#pyrkm.utils.generate_synthetic_data","title":"<code>generate_synthetic_data(target_entropy, data_size, structured=True)</code>","text":"<p>Generate synthetic data with the specified target entropy.</p> <p>Parameters:</p> <ul> <li> <code>target_entropy</code>               (<code>float</code>)           \u2013            <p>The target entropy value.</p> </li> <li> <code>data_size</code>               (<code>tuple</code>)           \u2013            <p>The size of the data to generate.</p> </li> <li> <code>structured</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, generate structured data (default is True).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The generated synthetic data.</p> </li> </ul> Source code in <code>src/pyrkm/utils.py</code> <pre><code>def generate_synthetic_data(target_entropy, data_size, structured=True):\n    \"\"\"Generate synthetic data with the specified target entropy.\n\n    Parameters\n    ----------\n    target_entropy : float\n        The target entropy value.\n    data_size : tuple\n        The size of the data to generate.\n    structured : bool, optional\n        If True, generate structured data (default is True).\n\n    Returns\n    -------\n    numpy.ndarray\n        The generated synthetic data.\n    \"\"\"\n\n    def S_lambda(x):\n        return -x * np.nan_to_num(np.log2(x)) - (1 - x) * np.nan_to_num(\n            np.log2(1 - x))\n\n    pixel_target = generate_S_matrix((data_size[1], data_size[2]),\n                                     target_entropy)\n    if structured:\n        flat_indices = np.argsort(pixel_target.flatten())\n        pixel_target = pixel_target.flatten()[flat_indices].reshape(\n            pixel_target.shape)\n\n    initial_guess = np.zeros((data_size[1], data_size[2]))\n    P = fsolve(lambda x: S_lambda(x) - pixel_target.flatten(),\n               initial_guess.flatten())\n\n    generated_data = ((np.random.rand(data_size[0],\n                                      data_size[1] * data_size[2])\n                       &lt; P).astype(int)).reshape(data_size)\n    S_image, S_pixel = my_entropy(generated_data)\n    print(f'\\nTarget = {target_entropy}')\n    print(f'generated entropy (image) = {S_image.mean()}')\n    print(f'generated entropy (pixels) = {S_pixel.mean()}')\n\n    return generated_data\n</code></pre>"},{"location":"api/utils/#pyrkm.utils.getbasebias","title":"<code>getbasebias(data)</code>","text":"<p>Returns the maximum likelihood estimate of the visible bias, given the data. If no data is given the RBMs bias value is return, but is highly recommended to pass the data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>array - like</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>The base bias.</p> </li> </ul> Source code in <code>src/pyrkm/utils.py</code> <pre><code>def getbasebias(data):\n    \"\"\"Returns the maximum likelihood estimate of the visible bias,\n    given the data. If no data is given the RBMs bias value is return,\n    but is highly recommended to pass the data.\n\n    Parameters\n    ----------\n    data : array-like\n        The input data.\n\n    Returns\n    -------\n    torch.Tensor\n        The base bias.\n    \"\"\"\n    save_mean = torch.clip(data.mean(0), 0.00001, 0.99999)\n    return (torch.log(save_mean) - torch.log(1.0 - save_mean))\n</code></pre>"},{"location":"api/utils/#pyrkm.utils.load_model","title":"<code>load_model(name, delete_previous=False, model_state_path='model_states/')</code>","text":"<p>Load a model from the specified path.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the model to load.</p> </li> <li> <code>delete_previous</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, delete previous model checkpoints except the latest one (default is False).</p> </li> <li> <code>model_state_path</code>               (<code>str</code>, default:                   <code>'model_states/'</code> )           \u2013            <p>The path to the model state directory (default is 'model_states/').</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if the model is loaded successfully, False otherwise.</p> </li> <li> <code>object</code>           \u2013            <p>The loaded model if successful, otherwise an empty list.</p> </li> </ul> Source code in <code>src/pyrkm/utils.py</code> <pre><code>def load_model(name, delete_previous=False, model_state_path='model_states/'):\n    \"\"\"Load a model from the specified path.\n\n    Parameters\n    ----------\n    name : str\n        The name of the model to load.\n    delete_previous : bool, optional\n        If True, delete previous model checkpoints except the latest one (default is False).\n    model_state_path : str, optional\n        The path to the model state directory (default is 'model_states/').\n\n    Returns\n    -------\n    bool\n        True if the model is loaded successfully, False otherwise.\n    object\n        The loaded model if successful, otherwise an empty list.\n    \"\"\"\n    # Check if you have model load points\n    filename_list = glob.glob(model_state_path + '{}_t*.pkl'.format(name))\n    if len(filename_list) &gt; 0:\n        all_loadpoints = sorted(\n            [int(x.split('_t')[-1].split('.pkl')[0]) for x in filename_list])\n        last_epoch = all_loadpoints[-1]\n        print('** Model {} trained up to epoch {}, so I load it'.format(\n            name, last_epoch),\n              flush=True)\n        with open(model_state_path + '{}_t{}.pkl'.format(name, last_epoch),\n                  'rb') as file:\n            model = pickle.load(file)\n        if delete_previous:\n            # Remove all the previous loadpoints\n            for x in all_loadpoints[:-1]:\n                os.remove(model_state_path + '{}_t{}.pkl'.format(name, x))\n        return True, model\n    else:\n        print('** No load points for {}'.format(name), flush=True)\n        return False, []\n</code></pre>"},{"location":"api/utils/#pyrkm.utils.make_grid","title":"<code>make_grid(array, nrow=8, padding=2)</code>","text":"<p>Create a grid of images.</p> <p>Parameters:</p> <ul> <li> <code>array</code>               (<code>array - like</code>)           \u2013            <p>The array of images.</p> </li> <li> <code>nrow</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The number of images in each row (default is 8).</p> </li> <li> <code>padding</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>The amount of padding between images (default is 2).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>array - like</code>           \u2013            <p>The grid of images.</p> </li> </ul> Source code in <code>src/pyrkm/utils.py</code> <pre><code>def make_grid(array, nrow=8, padding=2):\n    \"\"\"Create a grid of images.\n\n    Parameters\n    ----------\n    array : array-like\n        The array of images.\n    nrow : int, optional\n        The number of images in each row (default is 8).\n    padding : int, optional\n        The amount of padding between images (default is 2).\n\n    Returns\n    -------\n    array-like\n        The grid of images.\n    \"\"\"\n    N = array.shape[0]\n    H = array.shape[1]\n    W = array.shape[2]\n    grid_h = int(np.ceil(N / float(nrow)))\n    grid_w = nrow\n    grid = np.zeros(\n        [grid_h * (H + padding) + padding, grid_w * (W + padding) + padding])\n    k = 0\n    for y in range(grid_h):\n        for x in range(grid_w):\n            if k &lt; N:\n                grid[y * (H + padding):y * (H + padding) + H,\n                     x * (W + padding):x * (W + padding) + W] = array[k]\n                k = k + 1\n    return grid\n</code></pre>"},{"location":"api/utils/#pyrkm.utils.my_entropy","title":"<code>my_entropy(data)</code>","text":"<p>Compute the entropy per image and per pixel.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The entropy per image.</p> </li> <li> <code>ndarray</code>           \u2013            <p>The entropy per pixel.</p> </li> </ul> Source code in <code>src/pyrkm/utils.py</code> <pre><code>def my_entropy(data):\n    \"\"\"Compute the entropy per image and per pixel.\n\n    Parameters\n    ----------\n    data : numpy.ndarray\n        The input data.\n\n    Returns\n    -------\n    numpy.ndarray\n        The entropy per image.\n    numpy.ndarray\n        The entropy per pixel.\n    \"\"\"\n    X = data.mean(1).mean(1)\n    S_image = -X * np.nan_to_num(np.log2(X)) - (1 - X) * np.nan_to_num(\n        np.log2(1 - X))\n    Y = data.mean(0)\n    S_pixel = -Y * np.nan_to_num(np.log2(Y)) - (1 - Y) * np.nan_to_num(\n        np.log2(1 - Y))\n    return S_image, S_pixel\n</code></pre>"},{"location":"api/utils/#pyrkm.utils.show_and_save","title":"<code>show_and_save(file_name, img, cmap='gray', vmin=None, vmax=None, save=False, savename='')</code>","text":"<p>Display and optionally save an image.</p> <p>Parameters:</p> <ul> <li> <code>file_name</code>               (<code>str</code>)           \u2013            <p>The title of the image.</p> </li> <li> <code>img</code>               (<code>array - like</code>)           \u2013            <p>The image data.</p> </li> <li> <code>cmap</code>               (<code>str</code>, default:                   <code>'gray'</code> )           \u2013            <p>The colormap to use (default is 'gray').</p> </li> <li> <code>vmin</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>The minimum data value that corresponds to colormap (default is None).</p> </li> <li> <code>vmax</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>The maximum data value that corresponds to colormap (default is None).</p> </li> <li> <code>save</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, save the image to a file (default is False).</p> </li> <li> <code>savename</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The name of the file to save the image (default is '').</p> </li> </ul> Source code in <code>src/pyrkm/utils.py</code> <pre><code>def show_and_save(file_name,\n                  img,\n                  cmap='gray',\n                  vmin=None,\n                  vmax=None,\n                  save=False,\n                  savename=''):\n    \"\"\"Display and optionally save an image.\n\n    Parameters\n    ----------\n    file_name : str\n        The title of the image.\n    img : array-like\n        The image data.\n    cmap : str, optional\n        The colormap to use (default is 'gray').\n    vmin : float, optional\n        The minimum data value that corresponds to colormap (default is None).\n    vmax : float, optional\n        The maximum data value that corresponds to colormap (default is None).\n    save : bool, optional\n        If True, save the image to a file (default is False).\n    savename : str, optional\n        The name of the file to save the image (default is '').\n    \"\"\"\n    plt.title(file_name)\n    plt.imshow(img, cmap=cmap, vmin=vmin, vmax=vmax)\n    if save:\n        plt.savefig(savename)\n        plt.close()\n    else:\n        plt.show()\n</code></pre>"},{"location":"api/utils/#pyrkm.utils.unpickle","title":"<code>unpickle(file)</code>","text":"<p>Unpickle a file.</p> <p>Parameters:</p> <ul> <li> <code>file</code>               (<code>str</code>)           \u2013            <p>The file to unpickle.</p> </li> </ul> Source code in <code>src/pyrkm/utils.py</code> <pre><code>def unpickle(file):\n    \"\"\"Unpickle a file.\n\n    Parameters\n    ----------\n    file : str\n        The file to unpickle.\n    \"\"\"\n    with open(file, 'rb') as fo:\n        dict = pickle.load(fo, encoding='bytes')\n    return dict\n</code></pre>"},{"location":"examples/first_example/","title":"How to train a Restricted Kirchhoff Machine","text":"<p>First we import the required libraries.</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport pickle\nimport os\nimport glob\nimport pandas as pd\nimport sys\nimport pyrkm\nimport torch\n</pre> import numpy as np import matplotlib.pyplot as plt import matplotlib as mpl import pickle import os import glob import pandas as pd import sys import pyrkm import torch <p>We now set the desired data type and device for the computations.</p> In\u00a0[2]: Copied! <pre># Torch operations: set double type\ntorch.set_default_dtype(torch.float64)\n\n# Select which gpu you want to use if a multi-gpu system\ngpu_id = 0\nos.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif device == 'cpu':\n    # otherwise use multicore-CPU\n    torch.set_num_threads(24)\nprint(device)\n\n# Fix the seed for reproducibility\ntorch.manual_seed(42)\n</pre> # Torch operations: set double type torch.set_default_dtype(torch.float64)  # Select which gpu you want to use if a multi-gpu system gpu_id = 0 os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id) device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if device == 'cpu':     # otherwise use multicore-CPU     torch.set_num_threads(24) print(device)  # Fix the seed for reproducibility torch.manual_seed(42) <pre>cpu\n</pre> Out[2]: <pre>&lt;torch._C.Generator at 0x7789d97a3b90&gt;</pre> <p>Create the directories to store the data and the model.</p> In\u00a0[3]: Copied! <pre>pyrkm.ensure_dir('model_states')\npyrkm.ensure_dir('model_measure')\n</pre> pyrkm.ensure_dir('model_states') pyrkm.ensure_dir('model_measure') In\u00a0[4]: Copied! <pre># Load data\nqmnist = pyrkm.unpickle(\"../../datasets/MNIST-120k\")\ndata = qmnist['data']\ntrain_data = np.reshape(data, newshape=(data.shape[0], 784)) / 255\n</pre> # Load data qmnist = pyrkm.unpickle(\"../../datasets/MNIST-120k\") data = qmnist['data'] train_data = np.reshape(data, newshape=(data.shape[0], 784)) / 255 <pre>/tmp/ipykernel_53457/3680477073.py:4: DeprecationWarning: `newshape` keyword argument is deprecated, use `shape=...` or pass shape positionally instead. (deprecated in NumPy 2.1)\n  train_data = np.reshape(data, newshape=(data.shape[0], 784)) / 255\n</pre> <p>We now separate the data into training and testing sets and binarize the images. Following this Ref. we only use the first 10000 samples for training.</p> In\u00a0[5]: Copied! <pre>testing_data = train_data[:500, :]\ntesting_labels = qmnist['labels'][:500]\ntrain_data = train_data[500:]\ntrain_labels = qmnist['labels'][500:]\n\ntesting_data = torch.Tensor(testing_data)\n\n# Use only 10000 data as Literature\ntrain_data = torch.Tensor(train_data[:10000])\n# and make them already binary\ntrain_data = torch.where(train_data &gt; 0.5, 1.0, 0.0).to(torch.float)\ntesting_data = torch.where(testing_data &gt; 0.5, 1.0, 0.0).to(torch.float)\n</pre> testing_data = train_data[:500, :] testing_labels = qmnist['labels'][:500] train_data = train_data[500:] train_labels = qmnist['labels'][500:]  testing_data = torch.Tensor(testing_data)  # Use only 10000 data as Literature train_data = torch.Tensor(train_data[:10000]) # and make them already binary train_data = torch.where(train_data &gt; 0.5, 1.0, 0.0).to(torch.float) testing_data = torch.where(testing_data &gt; 0.5, 1.0, 0.0).to(torch.float) <p>Let's plot some samples to see how they look like:</p> In\u00a0[6]: Copied! <pre>examples = pyrkm.make_grid(np.reshape(train_data[:64], newshape=(-1, 28, 28)))\nplt.imshow(examples, cmap='gray')\n</pre> examples = pyrkm.make_grid(np.reshape(train_data[:64], newshape=(-1, 28, 28))) plt.imshow(examples, cmap='gray') <pre>/tmp/ipykernel_53457/3585741921.py:1: DeprecationWarning: `newshape` keyword argument is deprecated, use `shape=...` or pass shape positionally instead. (deprecated in NumPy 2.1)\n  examples = pyrkm.make_grid(np.reshape(train_data[:64], newshape=(-1, 28, 28)))\n</pre> Out[6]: <pre>&lt;matplotlib.image.AxesImage at 0x7788d1917d90&gt;</pre> In\u00a0[7]: Copied! <pre># Model parameters\nmax_epochs = 100000\nbatch_size = 64\nlr = 0.001\noptimizer = 'SGD'\nenergy_type = 'hopfield'\nnhidden = 50\nminWeight = -20\nmaxWeight = 20\nregul_to_use = 'l2'\nl1factor = 1e-2\nl2factor = 1e-2\nk = 1\ntrain_algo = 'PCD'\ncentering = False\nout_infix = 'test'\naverage_data = torch.Tensor(train_data).mean(0).to(torch.double)\nmodel_beta = 1\nmytype = torch.float32\n</pre> # Model parameters max_epochs = 100000 batch_size = 64 lr = 0.001 optimizer = 'SGD' energy_type = 'hopfield' nhidden = 50 minWeight = -20 maxWeight = 20 regul_to_use = 'l2' l1factor = 1e-2 l2factor = 1e-2 k = 1 train_algo = 'PCD' centering = False out_infix = 'test' average_data = torch.Tensor(train_data).mean(0).to(torch.double) model_beta = 1 mytype = torch.float32 <p>and perpare the data to be loaded efficiently</p> In\u00a0[8]: Copied! <pre># Set the data type\ntrain_data = train_data.to(mytype)\ntesting_data = testing_data.to(mytype)\n# Perepare the batchloader\ntrain_loader = torch.utils.data.DataLoader(train_data.to(device),\n                                           batch_size=batch_size,\n                                           shuffle=True,\n                                           drop_last=True)\n</pre> # Set the data type train_data = train_data.to(mytype) testing_data = testing_data.to(mytype) # Perepare the batchloader train_loader = torch.utils.data.DataLoader(train_data.to(device),                                            batch_size=batch_size,                                            shuffle=True,                                            drop_last=True) <p>Before creating the model, we give it a unique name that will identify univoquely the model in the filesystem.</p> In\u00a0[9]: Copied! <pre># Define the model name\nmodel_name = 'nh{}_{}{}_'.format(nhidden, energy_type, max_epochs)\nif model_beta &lt; 1000:\n    model_name = '{}beta{}_'.format(model_name, model_beta)\nelse:\n    model_name = '{}Deterministic_'.format(model_name)\nif regul_to_use == 'l2':\n    model_name = '{}l2{}_'.format(model_name, l2factor)\nif regul_to_use == 'l1':\n    model_name = '{}l1{}_'.format(model_name, l1factor)\nmodel_name = '{}{}{}_{}_lr{}_bs{}_{}'.format(model_name, train_algo, k,\n                                             optimizer, lr, batch_size,\n                                             out_infix)\nprint('*** {} ***'.format(model_name), flush=True)\n</pre> # Define the model name model_name = 'nh{}_{}{}_'.format(nhidden, energy_type, max_epochs) if model_beta &lt; 1000:     model_name = '{}beta{}_'.format(model_name, model_beta) else:     model_name = '{}Deterministic_'.format(model_name) if regul_to_use == 'l2':     model_name = '{}l2{}_'.format(model_name, l2factor) if regul_to_use == 'l1':     model_name = '{}l1{}_'.format(model_name, l1factor) model_name = '{}{}{}_{}_lr{}_bs{}_{}'.format(model_name, train_algo, k,                                              optimizer, lr, batch_size,                                              out_infix) print('*** {} ***'.format(model_name), flush=True) <pre>*** nh50_hopfield100000_beta1_l20.01_PCD1_SGD_lr0.001_bs64_test ***\n</pre> <p>After checking that the model does not already exist, we initialize the model</p> In\u00a0[10]: Copied! <pre># *****  Load the model and Train\n# check if previous save point is available\nis_loadable, model = pyrkm.load_model(model_name, delete_previous=False)\nif not is_loadable:\n    # *** Initialize\n    model = pyrkm.RKM(model_name=model_name,\n                      n_visible=784,\n                      n_hidden=nhidden,\n                      k=k,\n                      lr=lr,\n                      max_epochs=max_epochs,\n                      energy_type=energy_type,\n                      optimizer=optimizer,\n                      batch_size=batch_size,\n                      train_algo=train_algo,\n                      centering=centering,\n                      average_data=average_data,\n                      model_beta=model_beta,\n                      mytype=mytype)\n</pre> # *****  Load the model and Train # check if previous save point is available is_loadable, model = pyrkm.load_model(model_name, delete_previous=False) if not is_loadable:     # *** Initialize     model = pyrkm.RKM(model_name=model_name,                       n_visible=784,                       n_hidden=nhidden,                       k=k,                       lr=lr,                       max_epochs=max_epochs,                       energy_type=energy_type,                       optimizer=optimizer,                       batch_size=batch_size,                       train_algo=train_algo,                       centering=centering,                       average_data=average_data,                       model_beta=model_beta,                       mytype=mytype) <pre>** Model nh50_hopfield100000_beta1_l20.01_PCD1_SGD_lr0.001_bs64_test trained up to epoch 100000, so I load it\n</pre> <p>Notice that it is possible to initialize the model from the trained state of another one with a compatible architecture. To do so you can uncomment the block below.</p> In\u00a0[11]: Copied! <pre>#if not is_loadable:\n#    pretrain_source = 'another_trained_compatible_model'\n#    model.pretrain(pretrain_source)\n</pre> #if not is_loadable: #    pretrain_source = 'another_trained_compatible_model' #    model.pretrain(pretrain_source) <p>We can finally perform the training!</p> In\u00a0[12]: Copied! <pre>model.train(train_loader, testing_data.to(model.device), print_error=True)\n</pre> model.train(train_loader, testing_data.to(model.device), print_error=True) <pre>*** Training finished\n</pre> In\u00a0[13]: Copied! <pre>if hasattr(model, 'energy_type'):\n    # Visualize the weights using a grid. Each grid contains the weights for a single hidden unit\n    model.plot_weights(max_epochs)\n    plt.show()\n    # The biases are plotted as a histogram\n    model.plot_bias(max_epochs)\n    plt.show()\n</pre> if hasattr(model, 'energy_type'):     # Visualize the weights using a grid. Each grid contains the weights for a single hidden unit     model.plot_weights(max_epochs)     plt.show()     # The biases are plotted as a histogram     model.plot_bias(max_epochs)     plt.show() <p>We can then show the performance of the model in reconstructing the input data. It is interesting to see how the reconstruction appears for different number of gibbs steps (<code>k</code>). Notice that while <code>k</code> was fixed during training, it can be changed during inference.</p> In\u00a0[14]: Copied! <pre># Reconstruct some of the training data\ntesting_data = train_data[:64]\nk_gibbs_list = [1, 10]\nfor tg_multiple in k_gibbs_list:\n    v_original, v_reconstructed = model.reconstruct(testing_data,\n                                                    tg_multiple * k)\n    pyrkm.show_and_save(\"(train) reconstructed k={}\".format(tg_multiple * k),\n                        pyrkm.make_grid(\n                            np.reshape(v_reconstructed,\n                                       newshape=(-1, 28, 28))),\n                        save=False)\n    plt.show()\npyrkm.show_and_save(\n    \"(train) real\",\n    pyrkm.make_grid(np.reshape(v_original, newshape=(-1, 28, 28))))\nplt.show()\n</pre> # Reconstruct some of the training data testing_data = train_data[:64] k_gibbs_list = [1, 10] for tg_multiple in k_gibbs_list:     v_original, v_reconstructed = model.reconstruct(testing_data,                                                     tg_multiple * k)     pyrkm.show_and_save(\"(train) reconstructed k={}\".format(tg_multiple * k),                         pyrkm.make_grid(                             np.reshape(v_reconstructed,                                        newshape=(-1, 28, 28))),                         save=False)     plt.show() pyrkm.show_and_save(     \"(train) real\",     pyrkm.make_grid(np.reshape(v_original, newshape=(-1, 28, 28)))) plt.show() <pre>/tmp/ipykernel_53457/1932051823.py:9: DeprecationWarning: `newshape` keyword argument is deprecated, use `shape=...` or pass shape positionally instead. (deprecated in NumPy 2.1)\n  np.reshape(v_reconstructed,\n</pre> <pre>/tmp/ipykernel_53457/1932051823.py:15: DeprecationWarning: `newshape` keyword argument is deprecated, use `shape=...` or pass shape positionally instead. (deprecated in NumPy 2.1)\n  pyrkm.make_grid(np.reshape(v_original, newshape=(-1, 28, 28))))\n</pre> In\u00a0[15]: Copied! <pre>gsteps_list = [1, 2000]\nfor tg_multiple in gsteps_list:\n    v_generated = model.generate(64, tg_multiple * k, from_visible=True)\n    pyrkm.show_and_save(\n        \"generated k={}\".format(tg_multiple * k),\n        pyrkm.make_grid(np.reshape(v_generated, newshape=(-1, 28, 28))))\n    plt.show()\n</pre> gsteps_list = [1, 2000] for tg_multiple in gsteps_list:     v_generated = model.generate(64, tg_multiple * k, from_visible=True)     pyrkm.show_and_save(         \"generated k={}\".format(tg_multiple * k),         pyrkm.make_grid(np.reshape(v_generated, newshape=(-1, 28, 28))))     plt.show() <pre>/tmp/ipykernel_53457/3375143408.py:6: DeprecationWarning: `newshape` keyword argument is deprecated, use `shape=...` or pass shape positionally instead. (deprecated in NumPy 2.1)\n  pyrkm.make_grid(np.reshape(v_generated, newshape=(-1, 28, 28))))\n</pre> In\u00a0[16]: Copied! <pre># Get all the intermediate states\nfilename_list = glob.glob('model_states/{}_t*.pkl'.format(model_name))\nall_loadpoints = sorted(\n    [int(x.split('_t')[-1].split('.pkl')[0]) for x in filename_list])\n\n# Prepare a list of all the measures\nenergy_array = []\npower_forward_array = []\npower_backward_array = []\ntime_forward_array = []\ntime_backward_array = []\nav_time_forward_array = []\nav_time_backward_array = []\nrec_error_train_array = []\nrec_error_test_array = []\nentropy_array = []\nfid_array = []\n\n# Loop over all the loadpoints and measure the physical quantities\nfor t in all_loadpoints:\n    with open('model_states/{}_t{}.pkl'.format(model_name, t), \"rb\") as file:\n        model = pickle.load(file)\n        # physical measures\n        energy_array.append(model.energy.cpu().numpy())\n        power_forward_array.append(model.power_f.cpu().numpy())\n        power_backward_array.append(model.power_b.cpu().numpy())\n        t_f = model.relax_t_f.cpu().numpy()\n        t_b = model.relax_t_b.cpu().numpy()\n        time_forward_array.append(t_f)\n        time_backward_array.append(t_b)\n        av_time_forward_array.append(t_f.mean())\n        av_time_backward_array.append(t_b.mean())\n\n        # errors\n        k = model.k\n        # Compute reconstruction error (averaged over pixels and samples)\n        v_model = model.forward(train_data.to(model.device), k)\n        rec_error_train = ((v_model -\n                            train_data.to(model.device))**2).mean(1).mean(0)\n        # Now compute the reconstruction error for the test set only\n        v_model = model.forward(testing_data.to(model.device), k)\n        rec_error_test = ((v_model -\n                           testing_data.to(model.device))**2).mean(1).mean(0)\n        rec_error_train_array.append(rec_error_train.cpu().numpy())\n        rec_error_test_array.append(rec_error_test.cpu().numpy())\n\n        # Entropy\n        v_generated = model.generate(testing_data.shape[0],\n                                     k,\n                                     from_visible=True)\n        entropy_array.append(\n            pyrkm.Compute_S(testing_data.to(model.device), v_generated))\n\n        # FID\n        fid = pyrkm.Compute_FID(testing_data.to(model.device), v_generated)\n        fid_array.append(fid)\n\n# transform to np array\nenergy_array = np.array(energy_array)\npower_forward_array = np.array(power_forward_array)\npower_backward_array = np.array(power_backward_array)\ntime_forward_array = np.array(time_forward_array)\ntime_backward_array = np.array(time_backward_array)\nav_time_forward_array = np.array(av_time_forward_array)\nav_time_backward_array = np.array(av_time_backward_array)\nrec_error_train_array = np.array(rec_error_train_array)\nrec_error_test_array = np.array(rec_error_test_array)\nentropy_array = np.array(entropy_array)\nfid_array = np.array(fid_array)\n</pre> # Get all the intermediate states filename_list = glob.glob('model_states/{}_t*.pkl'.format(model_name)) all_loadpoints = sorted(     [int(x.split('_t')[-1].split('.pkl')[0]) for x in filename_list])  # Prepare a list of all the measures energy_array = [] power_forward_array = [] power_backward_array = [] time_forward_array = [] time_backward_array = [] av_time_forward_array = [] av_time_backward_array = [] rec_error_train_array = [] rec_error_test_array = [] entropy_array = [] fid_array = []  # Loop over all the loadpoints and measure the physical quantities for t in all_loadpoints:     with open('model_states/{}_t{}.pkl'.format(model_name, t), \"rb\") as file:         model = pickle.load(file)         # physical measures         energy_array.append(model.energy.cpu().numpy())         power_forward_array.append(model.power_f.cpu().numpy())         power_backward_array.append(model.power_b.cpu().numpy())         t_f = model.relax_t_f.cpu().numpy()         t_b = model.relax_t_b.cpu().numpy()         time_forward_array.append(t_f)         time_backward_array.append(t_b)         av_time_forward_array.append(t_f.mean())         av_time_backward_array.append(t_b.mean())          # errors         k = model.k         # Compute reconstruction error (averaged over pixels and samples)         v_model = model.forward(train_data.to(model.device), k)         rec_error_train = ((v_model -                             train_data.to(model.device))**2).mean(1).mean(0)         # Now compute the reconstruction error for the test set only         v_model = model.forward(testing_data.to(model.device), k)         rec_error_test = ((v_model -                            testing_data.to(model.device))**2).mean(1).mean(0)         rec_error_train_array.append(rec_error_train.cpu().numpy())         rec_error_test_array.append(rec_error_test.cpu().numpy())          # Entropy         v_generated = model.generate(testing_data.shape[0],                                      k,                                      from_visible=True)         entropy_array.append(             pyrkm.Compute_S(testing_data.to(model.device), v_generated))          # FID         fid = pyrkm.Compute_FID(testing_data.to(model.device), v_generated)         fid_array.append(fid)  # transform to np array energy_array = np.array(energy_array) power_forward_array = np.array(power_forward_array) power_backward_array = np.array(power_backward_array) time_forward_array = np.array(time_forward_array) time_backward_array = np.array(time_backward_array) av_time_forward_array = np.array(av_time_forward_array) av_time_backward_array = np.array(av_time_backward_array) rec_error_train_array = np.array(rec_error_train_array) rec_error_test_array = np.array(rec_error_test_array) entropy_array = np.array(entropy_array) fid_array = np.array(fid_array) <pre>/home/simone/.virtualenvs/pyrkm/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/home/simone/.virtualenvs/pyrkm/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n</pre> <p>We can finally plot the observables to see how the model is learning.</p> In\u00a0[17]: Copied! <pre>fig, axs = plt.subplots(6, 1, figsize=(10, 10), sharex=True, tight_layout=True)\ncmap = plt.get_cmap('Set1')\ncolors = [cmap(i) for i in range(7)]\naxs[0].plot(all_loadpoints,\n            rec_error_train_array,\n            '-o',\n            label='train',\n            color=colors[0])\naxs[0].plot(all_loadpoints,\n            rec_error_test_array,\n            '-o',\n            label='test',\n            color=colors[6])\naxs[1].plot(all_loadpoints,\n            av_time_forward_array,\n            '-o',\n            label='forward',\n            color=colors[0])\naxs[1].plot(all_loadpoints,\n            av_time_backward_array,\n            '-o',\n            label='backward',\n            color=colors[1])\naxs[2].plot(all_loadpoints,\n            power_forward_array,\n            '-o',\n            label='forward',\n            color=colors[0])\naxs[2].plot(all_loadpoints,\n            power_backward_array,\n            '-o',\n            label='backward',\n            color=colors[1])\naxs[3].plot(all_loadpoints,\n            energy_array,\n            '-o',\n            label='energy',\n            color=colors[2])\naxs[3].plot(all_loadpoints,\n            energy_array * (av_time_backward_array + av_time_forward_array) /\n            2,\n            '-o',\n            label='energy*time',\n            color=colors[3])\naxs[4].plot(all_loadpoints,\n            entropy_array,\n            '-o',\n            label='entropy',\n            color=colors[-1])\naxs[5].plot(all_loadpoints, fid_array, '-o', label='FID', color=colors[-2])\n\naxs[0].set_ylabel('&lt;rec. error&gt;')\naxs[1].set_ylabel('&lt;relaxation time&gt;')\naxs[2].set_ylabel('&lt;power&gt;')\naxs[3].set_ylabel('energy')\naxs[4].set_ylabel('entropy')\naxs[5].set_ylabel('fid')\n\n[ax.legend() for ax in axs]\n\naxs[-1].set_xlabel('epoch')\n\n[ax.set_yscale('log') for ax in axs[1:]]\n[ax.set_xscale('log') for ax in axs]\n[ax.grid() for ax in axs]\n\nplt.show()\n</pre> fig, axs = plt.subplots(6, 1, figsize=(10, 10), sharex=True, tight_layout=True) cmap = plt.get_cmap('Set1') colors = [cmap(i) for i in range(7)] axs[0].plot(all_loadpoints,             rec_error_train_array,             '-o',             label='train',             color=colors[0]) axs[0].plot(all_loadpoints,             rec_error_test_array,             '-o',             label='test',             color=colors[6]) axs[1].plot(all_loadpoints,             av_time_forward_array,             '-o',             label='forward',             color=colors[0]) axs[1].plot(all_loadpoints,             av_time_backward_array,             '-o',             label='backward',             color=colors[1]) axs[2].plot(all_loadpoints,             power_forward_array,             '-o',             label='forward',             color=colors[0]) axs[2].plot(all_loadpoints,             power_backward_array,             '-o',             label='backward',             color=colors[1]) axs[3].plot(all_loadpoints,             energy_array,             '-o',             label='energy',             color=colors[2]) axs[3].plot(all_loadpoints,             energy_array * (av_time_backward_array + av_time_forward_array) /             2,             '-o',             label='energy*time',             color=colors[3]) axs[4].plot(all_loadpoints,             entropy_array,             '-o',             label='entropy',             color=colors[-1]) axs[5].plot(all_loadpoints, fid_array, '-o', label='FID', color=colors[-2])  axs[0].set_ylabel('') axs[1].set_ylabel('') axs[2].set_ylabel('') axs[3].set_ylabel('energy') axs[4].set_ylabel('entropy') axs[5].set_ylabel('fid')  [ax.legend() for ax in axs]  axs[-1].set_xlabel('epoch')  [ax.set_yscale('log') for ax in axs[1:]] [ax.set_xscale('log') for ax in axs] [ax.grid() for ax in axs]  plt.show() <p>You reached the end of this tutorial! I hope you enjoyed it and that you are now ready to train your own RKM. If you have any question, feel free to contact one of the developers or open an issue on the GitHub repository</p>"},{"location":"examples/first_example/#how-to-train-a-restricted-kirchhoff-machine","title":"How to train a Restricted Kirchhoff Machine\u00b6","text":"<p>Welcome to pyrkm! In this example I will guide you through the process of training a Restricted Kirchhoff Machine (RKM) on the MNIST dataset.</p>"},{"location":"examples/first_example/#load-the-data","title":"Load the data\u00b6","text":"<p>For this example we will use the MNIST dataset. We have created a submodule that contains some cleaned dataset that have been testes. If you have cloned the <code>pyrkm</code> repository, be sure that you have initialized the submodule correctly.</p>"},{"location":"examples/first_example/#training","title":"Training\u00b6","text":"<p>To train the RKM we need to define the architecture of the model and the hyperparameters. In the block below we make a standard choice that will produce acceptable results for this example. If you want to know more about the specific parameters you can look at the documentation of the <code>RKM</code> class.</p>"},{"location":"examples/first_example/#visualization","title":"Visualization\u00b6","text":"<p><code>pyrkm</code> provides some tools to visualize the training process and give you an idea of how the model is learning. First, we can plot weights and biases of the model:</p>"},{"location":"examples/first_example/#generative-performance","title":"Generative performance\u00b6","text":"<p>We now look at how good the model is at generating new samples. Notice that usually in studies about RBM generative features, it is often reported the persistent chain as 'generated' output. Here instead we generate new samples in real time starting from random noise, to mimick the real use case of the RKM:</p>"},{"location":"examples/first_example/#measures","title":"Measures\u00b6","text":"<p>We can now measure and plot the most interesting observables to asses the model performance. We will measure multiple observables at the same time, and with a similar loop you will be able to reproduce the plots reported in XXX that quantify the performance improvement during training.</p>"}]}